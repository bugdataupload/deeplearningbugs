WARNING:tensorflow:From train.py:196: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

WARNING:tensorflow:From train.py:54: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
W0209 01:22:47.076346 4380278208 deprecation.py:323] From train.py:54: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
W0209 01:22:47.076677 4380278208 deprecation.py:323] From /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
W0209 01:22:47.076968 4380278208 deprecation.py:323] From /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From train.py:80: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0209 01:22:47.860138 4380278208 module_wrapper.py:139] From train.py:80: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From train.py:83: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

W0209 01:22:47.861655 4380278208 module_wrapper.py:139] From train.py:83: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-02-09 01:22:47.871722: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-09 01:22:47.911332: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff9b9e9f450 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-09 01:22:47.911358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:15: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0209 01:22:47.923754 4380278208 module_wrapper.py:139] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:15: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:25: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0209 01:22:47.950582 4380278208 module_wrapper.py:139] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:25: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:36: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

W0209 01:22:47.990505 4380278208 module_wrapper.py:139] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:36: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:47: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

W0209 01:22:48.014661 4380278208 module_wrapper.py:139] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:47: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0209 01:22:48.055382 4380278208 deprecation.py:506] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:66: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0209 01:22:48.072354 4380278208 module_wrapper.py:139] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:66: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0209 01:22:48.072575 4380278208 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:73: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.

W0209 01:22:48.089637 4380278208 module_wrapper.py:139] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:73: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.

WARNING:tensorflow:From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

W0209 01:22:48.098212 4380278208 deprecation.py:323] From /Volumes/Transcend/Mutation/CNN-TF/text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From train.py:96: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0209 01:22:48.185221 4380278208 module_wrapper.py:139] From train.py:96: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From train.py:104: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

W0209 01:22:48.588144 4380278208 module_wrapper.py:139] From train.py:104: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.
I0209 01:22:48.588372 4380278208 summary_op_util.py:66] Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.
WARNING:tensorflow:From train.py:105: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0209 01:22:48.603852 4380278208 module_wrapper.py:139] From train.py:105: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.
I0209 01:22:48.653733 4380278208 summary_op_util.py:66] Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.
INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.
I0209 01:22:48.655050 4380278208 summary_op_util.py:66] Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.
INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.
I0209 01:22:48.693518 4380278208 summary_op_util.py:66] Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.
INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.
I0209 01:22:48.695248 4380278208 summary_op_util.py:66] Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.
INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.
I0209 01:22:48.730074 4380278208 summary_op_util.py:66] Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.
INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.
I0209 01:22:48.733229 4380278208 summary_op_util.py:66] Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.
INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.
I0209 01:22:48.761207 4380278208 summary_op_util.py:66] Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.
INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.
I0209 01:22:48.764036 4380278208 summary_op_util.py:66] Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.
INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.
I0209 01:22:48.793119 4380278208 summary_op_util.py:66] Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.
INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.
I0209 01:22:48.794255 4380278208 summary_op_util.py:66] Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.
INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.
I0209 01:22:48.823963 4380278208 summary_op_util.py:66] Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.
INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.
I0209 01:22:48.825289 4380278208 summary_op_util.py:66] Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.
INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.
I0209 01:22:48.855216 4380278208 summary_op_util.py:66] Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.
INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.
I0209 01:22:48.857069 4380278208 summary_op_util.py:66] Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.
INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.
I0209 01:22:48.888920 4380278208 summary_op_util.py:66] Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.
INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.
I0209 01:22:48.889968 4380278208 summary_op_util.py:66] Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.
INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.
I0209 01:22:48.919545 4380278208 summary_op_util.py:66] Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.
WARNING:tensorflow:From train.py:108: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.

W0209 01:22:48.922837 4380278208 module_wrapper.py:139] From train.py:108: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.

WARNING:tensorflow:From train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

W0209 01:22:48.933252 4380278208 module_wrapper.py:139] From train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W0209 01:22:49.370944 4380278208 module_wrapper.py:139] From train.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:134: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W0209 01:22:49.371524 4380278208 module_wrapper.py:139] From train.py:134: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From train.py:140: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

W0209 01:22:49.502047 4380278208 module_wrapper.py:139] From train.py:140: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From train.py:182: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

W0209 01:22:51.040168 4380278208 module_wrapper.py:139] From train.py:182: The name tf.train.global_step is deprecated. Please use tf.compat.v1.train.global_step instead.

WARNING:tensorflow:From /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
W0209 01:25:04.184576 4380278208 deprecation.py:323] From /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
2020-02-09 01:27:18.991150: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:109 : Not found: /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints; No such file or directory
Loading data...
Vocabulary Size: 18758
Train/Dev split: 9596/1066
Writing to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568

2020-02-09T01:22:51.038798: step 1, loss 2.02115, acc 0.421875
2020-02-09T01:22:51.274061: step 2, loss 1.74387, acc 0.5
2020-02-09T01:22:51.452141: step 3, loss 1.43343, acc 0.59375
2020-02-09T01:22:51.628773: step 4, loss 2.14884, acc 0.453125
2020-02-09T01:22:51.803235: step 5, loss 1.92635, acc 0.5
2020-02-09T01:22:51.994094: step 6, loss 1.9539, acc 0.515625
2020-02-09T01:22:52.174815: step 7, loss 1.67972, acc 0.453125
2020-02-09T01:22:52.350004: step 8, loss 2.20756, acc 0.515625
2020-02-09T01:22:52.551936: step 9, loss 2.24163, acc 0.4375
2020-02-09T01:22:52.724181: step 10, loss 1.30158, acc 0.59375
2020-02-09T01:22:52.898345: step 11, loss 2.10833, acc 0.453125
2020-02-09T01:22:53.070620: step 12, loss 1.78204, acc 0.546875
2020-02-09T01:22:53.259068: step 13, loss 1.77843, acc 0.5625
2020-02-09T01:22:53.440643: step 14, loss 1.89389, acc 0.546875
2020-02-09T01:22:53.642846: step 15, loss 1.98028, acc 0.546875
2020-02-09T01:22:53.845305: step 16, loss 2.20601, acc 0.453125
2020-02-09T01:22:54.117814: step 17, loss 2.01614, acc 0.484375
2020-02-09T01:22:54.376532: step 18, loss 1.95716, acc 0.453125
2020-02-09T01:22:54.612090: step 19, loss 2.07773, acc 0.484375
2020-02-09T01:22:54.874351: step 20, loss 1.67932, acc 0.515625
2020-02-09T01:22:55.151751: step 21, loss 2.40876, acc 0.4375
2020-02-09T01:22:55.394414: step 22, loss 1.5403, acc 0.578125
2020-02-09T01:22:55.627230: step 23, loss 1.61014, acc 0.546875
2020-02-09T01:22:55.851509: step 24, loss 2.17463, acc 0.390625
2020-02-09T01:22:56.097431: step 25, loss 1.32409, acc 0.515625
2020-02-09T01:22:56.300044: step 26, loss 2.26377, acc 0.390625
2020-02-09T01:22:56.514424: step 27, loss 1.57385, acc 0.484375
2020-02-09T01:22:56.717178: step 28, loss 1.77489, acc 0.484375
2020-02-09T01:22:56.917704: step 29, loss 1.43999, acc 0.53125
2020-02-09T01:22:57.136294: step 30, loss 1.88041, acc 0.5
2020-02-09T01:22:57.350442: step 31, loss 1.95751, acc 0.53125
2020-02-09T01:22:57.563021: step 32, loss 2.08519, acc 0.390625
2020-02-09T01:22:57.769586: step 33, loss 1.78477, acc 0.484375
2020-02-09T01:22:57.962258: step 34, loss 1.87558, acc 0.390625
2020-02-09T01:22:58.191231: step 35, loss 1.67401, acc 0.609375
2020-02-09T01:22:58.419755: step 36, loss 2.05206, acc 0.34375
2020-02-09T01:22:58.644340: step 37, loss 1.25307, acc 0.609375
2020-02-09T01:22:58.838322: step 38, loss 1.36326, acc 0.5625
2020-02-09T01:22:59.067259: step 39, loss 2.11379, acc 0.421875
2020-02-09T01:22:59.286488: step 40, loss 1.97168, acc 0.484375
2020-02-09T01:22:59.494314: step 41, loss 1.81604, acc 0.484375
2020-02-09T01:22:59.701818: step 42, loss 1.45827, acc 0.453125
2020-02-09T01:22:59.928616: step 43, loss 1.82775, acc 0.5625
2020-02-09T01:23:00.176446: step 44, loss 1.50453, acc 0.484375
2020-02-09T01:23:00.399168: step 45, loss 2.08539, acc 0.40625
2020-02-09T01:23:00.593711: step 46, loss 1.68731, acc 0.515625
2020-02-09T01:23:00.814299: step 47, loss 1.82993, acc 0.46875
2020-02-09T01:23:01.031318: step 48, loss 1.40218, acc 0.53125
2020-02-09T01:23:01.231714: step 49, loss 1.193, acc 0.625
2020-02-09T01:23:01.434418: step 50, loss 2.13745, acc 0.484375
2020-02-09T01:23:01.637077: step 51, loss 1.04294, acc 0.59375
2020-02-09T01:23:01.854786: step 52, loss 1.82182, acc 0.46875
2020-02-09T01:23:02.090230: step 53, loss 1.46675, acc 0.578125
2020-02-09T01:23:02.304109: step 54, loss 1.65516, acc 0.484375
2020-02-09T01:23:02.503473: step 55, loss 1.45116, acc 0.5625
2020-02-09T01:23:02.713450: step 56, loss 1.21997, acc 0.53125
2020-02-09T01:23:02.930240: step 57, loss 1.38246, acc 0.5
2020-02-09T01:23:03.148452: step 58, loss 1.50991, acc 0.53125
2020-02-09T01:23:03.327847: step 59, loss 1.63205, acc 0.546875
2020-02-09T01:23:03.530546: step 60, loss 1.50366, acc 0.53125
2020-02-09T01:23:03.700962: step 61, loss 1.30284, acc 0.609375
2020-02-09T01:23:03.890107: step 62, loss 1.69848, acc 0.46875
2020-02-09T01:23:04.147508: step 63, loss 1.25021, acc 0.609375
2020-02-09T01:23:04.407845: step 64, loss 1.04271, acc 0.59375
2020-02-09T01:23:04.619062: step 65, loss 1.30467, acc 0.515625
2020-02-09T01:23:04.846416: step 66, loss 1.34069, acc 0.53125
2020-02-09T01:23:05.075852: step 67, loss 1.34399, acc 0.59375
2020-02-09T01:23:05.316332: step 68, loss 1.55736, acc 0.5625
2020-02-09T01:23:05.557972: step 69, loss 1.4956, acc 0.484375
2020-02-09T01:23:05.761041: step 70, loss 1.34267, acc 0.46875
2020-02-09T01:23:05.982871: step 71, loss 1.23623, acc 0.59375
2020-02-09T01:23:06.199131: step 72, loss 1.94313, acc 0.53125
2020-02-09T01:23:06.418788: step 73, loss 1.50003, acc 0.5
2020-02-09T01:23:06.608414: step 74, loss 1.36335, acc 0.5625
2020-02-09T01:23:06.822680: step 75, loss 1.38463, acc 0.5
2020-02-09T01:23:07.037493: step 76, loss 1.33577, acc 0.515625
2020-02-09T01:23:07.213927: step 77, loss 1.39876, acc 0.40625
2020-02-09T01:23:07.419983: step 78, loss 1.50147, acc 0.5
2020-02-09T01:23:07.635369: step 79, loss 1.49608, acc 0.53125
2020-02-09T01:23:07.849325: step 80, loss 1.58691, acc 0.515625
2020-02-09T01:23:08.074983: step 81, loss 1.06043, acc 0.609375
2020-02-09T01:23:08.300529: step 82, loss 1.2787, acc 0.484375
2020-02-09T01:23:08.503369: step 83, loss 1.82264, acc 0.390625
2020-02-09T01:23:08.740231: step 84, loss 1.326, acc 0.515625
2020-02-09T01:23:09.042774: step 85, loss 1.49394, acc 0.421875
2020-02-09T01:23:09.254843: step 86, loss 1.32731, acc 0.546875
2020-02-09T01:23:09.478436: step 87, loss 1.31911, acc 0.53125
2020-02-09T01:23:09.709660: step 88, loss 1.55232, acc 0.53125
2020-02-09T01:23:09.955972: step 89, loss 1.20261, acc 0.546875
2020-02-09T01:23:10.189406: step 90, loss 1.46922, acc 0.5625
2020-02-09T01:23:10.430028: step 91, loss 1.41995, acc 0.5
2020-02-09T01:23:10.618738: step 92, loss 1.01341, acc 0.609375
2020-02-09T01:23:10.820613: step 93, loss 1.10452, acc 0.5625
2020-02-09T01:23:11.039179: step 94, loss 1.55995, acc 0.453125
2020-02-09T01:23:11.208358: step 95, loss 1.40377, acc 0.453125
2020-02-09T01:23:11.407430: step 96, loss 1.17553, acc 0.640625
2020-02-09T01:23:11.626666: step 97, loss 1.53647, acc 0.46875
2020-02-09T01:23:11.850075: step 98, loss 1.10273, acc 0.53125
2020-02-09T01:23:12.060027: step 99, loss 1.37933, acc 0.515625
2020-02-09T01:23:12.278667: step 100, loss 1.47972, acc 0.46875

Evaluation:
2020-02-09T01:23:12.747120: step 100, loss 0.921162, acc 0.538462

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-100

2020-02-09T01:23:15.124613: step 101, loss 1.09645, acc 0.703125
2020-02-09T01:23:15.369260: step 102, loss 1.22595, acc 0.515625
2020-02-09T01:23:15.568446: step 103, loss 1.22003, acc 0.546875
2020-02-09T01:23:15.757943: step 104, loss 1.6382, acc 0.515625
2020-02-09T01:23:15.946810: step 105, loss 1.2482, acc 0.515625
2020-02-09T01:23:16.151501: step 106, loss 1.67375, acc 0.375
2020-02-09T01:23:16.355885: step 107, loss 1.4876, acc 0.46875
2020-02-09T01:23:16.555983: step 108, loss 1.22129, acc 0.5
2020-02-09T01:23:16.767054: step 109, loss 1.38672, acc 0.546875
2020-02-09T01:23:16.958476: step 110, loss 0.919143, acc 0.6875
2020-02-09T01:23:17.162989: step 111, loss 1.34645, acc 0.5
2020-02-09T01:23:17.375055: step 112, loss 1.20353, acc 0.578125
2020-02-09T01:23:17.580962: step 113, loss 1.35487, acc 0.46875
2020-02-09T01:23:17.809364: step 114, loss 1.2552, acc 0.53125
2020-02-09T01:23:18.013101: step 115, loss 1.02483, acc 0.59375
2020-02-09T01:23:18.228904: step 116, loss 0.962922, acc 0.625
2020-02-09T01:23:18.446869: step 117, loss 1.19621, acc 0.484375
2020-02-09T01:23:18.665161: step 118, loss 1.17954, acc 0.5625
2020-02-09T01:23:18.829203: step 119, loss 1.23835, acc 0.515625
2020-02-09T01:23:19.037742: step 120, loss 1.54873, acc 0.59375
2020-02-09T01:23:19.209931: step 121, loss 1.11331, acc 0.578125
2020-02-09T01:23:19.406560: step 122, loss 1.26171, acc 0.484375
2020-02-09T01:23:19.613156: step 123, loss 1.0874, acc 0.5625
2020-02-09T01:23:19.848618: step 124, loss 1.1966, acc 0.578125
2020-02-09T01:23:20.097835: step 125, loss 1.09171, acc 0.5
2020-02-09T01:23:20.311060: step 126, loss 0.985063, acc 0.484375
2020-02-09T01:23:20.544854: step 127, loss 1.27441, acc 0.5625
2020-02-09T01:23:20.763455: step 128, loss 0.823672, acc 0.671875
2020-02-09T01:23:20.999034: step 129, loss 1.1864, acc 0.53125
2020-02-09T01:23:21.230086: step 130, loss 0.874721, acc 0.609375
2020-02-09T01:23:21.451787: step 131, loss 1.48826, acc 0.546875
2020-02-09T01:23:21.680854: step 132, loss 0.9709, acc 0.609375
2020-02-09T01:23:21.883754: step 133, loss 1.14663, acc 0.5625
2020-02-09T01:23:22.106391: step 134, loss 1.33178, acc 0.53125
2020-02-09T01:23:22.344054: step 135, loss 1.12543, acc 0.53125
2020-02-09T01:23:22.564757: step 136, loss 0.911784, acc 0.640625
2020-02-09T01:23:22.790977: step 137, loss 1.33508, acc 0.4375
2020-02-09T01:23:23.004952: step 138, loss 1.41367, acc 0.546875
2020-02-09T01:23:23.253676: step 139, loss 0.989165, acc 0.625
2020-02-09T01:23:23.473504: step 140, loss 1.39555, acc 0.421875
2020-02-09T01:23:23.689550: step 141, loss 1.15222, acc 0.578125
2020-02-09T01:23:23.959016: step 142, loss 1.36041, acc 0.484375
2020-02-09T01:23:24.200117: step 143, loss 1.05632, acc 0.5625
2020-02-09T01:23:24.417923: step 144, loss 1.16387, acc 0.578125
2020-02-09T01:23:24.646857: step 145, loss 1.50224, acc 0.40625
2020-02-09T01:23:24.919524: step 146, loss 0.951649, acc 0.671875
2020-02-09T01:23:25.145478: step 147, loss 1.0326, acc 0.609375
2020-02-09T01:23:25.387467: step 148, loss 1.29002, acc 0.5
2020-02-09T01:23:25.627842: step 149, loss 0.985752, acc 0.515625
2020-02-09T01:23:25.831250: step 150, loss 1.14288, acc 0.516667
2020-02-09T01:23:26.057866: step 151, loss 1.05069, acc 0.5625
2020-02-09T01:23:26.242955: step 152, loss 1.09565, acc 0.53125
2020-02-09T01:23:26.458723: step 153, loss 0.894451, acc 0.609375
2020-02-09T01:23:26.692868: step 154, loss 1.00486, acc 0.609375
2020-02-09T01:23:26.918971: step 155, loss 1.05217, acc 0.5625
2020-02-09T01:23:27.091140: step 156, loss 0.702259, acc 0.703125
2020-02-09T01:23:27.303830: step 157, loss 0.907263, acc 0.65625
2020-02-09T01:23:27.446153: step 158, loss 0.910688, acc 0.59375
2020-02-09T01:23:27.666405: step 159, loss 0.972831, acc 0.5625
2020-02-09T01:23:27.867203: step 160, loss 0.974047, acc 0.59375
2020-02-09T01:23:28.123544: step 161, loss 0.794133, acc 0.625
2020-02-09T01:23:28.335277: step 162, loss 1.04083, acc 0.640625
2020-02-09T01:23:28.553774: step 163, loss 0.715189, acc 0.703125
2020-02-09T01:23:28.730314: step 164, loss 1.17545, acc 0.578125
2020-02-09T01:23:28.944437: step 165, loss 0.891256, acc 0.578125
2020-02-09T01:23:29.159275: step 166, loss 0.890507, acc 0.59375
2020-02-09T01:23:29.361895: step 167, loss 0.647684, acc 0.734375
2020-02-09T01:23:29.568558: step 168, loss 1.12013, acc 0.546875
2020-02-09T01:23:29.777715: step 169, loss 0.818015, acc 0.6875
2020-02-09T01:23:30.012854: step 170, loss 0.998219, acc 0.609375
2020-02-09T01:23:30.230011: step 171, loss 0.654426, acc 0.6875
2020-02-09T01:23:30.449165: step 172, loss 0.79001, acc 0.59375
2020-02-09T01:23:30.652865: step 173, loss 1.16666, acc 0.515625
2020-02-09T01:23:30.893529: step 174, loss 0.846823, acc 0.578125
2020-02-09T01:23:31.102320: step 175, loss 1.00386, acc 0.609375
2020-02-09T01:23:31.304245: step 176, loss 0.785902, acc 0.578125
2020-02-09T01:23:31.492847: step 177, loss 0.91576, acc 0.6875
2020-02-09T01:23:31.686216: step 178, loss 0.875304, acc 0.6875
2020-02-09T01:23:31.871889: step 179, loss 0.858296, acc 0.5625
2020-02-09T01:23:32.057898: step 180, loss 1.00655, acc 0.578125
2020-02-09T01:23:32.247380: step 181, loss 1.07016, acc 0.515625
2020-02-09T01:23:32.436945: step 182, loss 1.07177, acc 0.515625
2020-02-09T01:23:32.627442: step 183, loss 0.820802, acc 0.671875
2020-02-09T01:23:32.828158: step 184, loss 0.800629, acc 0.640625
2020-02-09T01:23:33.019868: step 185, loss 0.926808, acc 0.578125
2020-02-09T01:23:33.211953: step 186, loss 0.708968, acc 0.65625
2020-02-09T01:23:33.395143: step 187, loss 0.795215, acc 0.71875
2020-02-09T01:23:33.587018: step 188, loss 0.837894, acc 0.5625
2020-02-09T01:23:33.772071: step 189, loss 0.975298, acc 0.6875
2020-02-09T01:23:33.972684: step 190, loss 0.930815, acc 0.640625
2020-02-09T01:23:34.166908: step 191, loss 0.877239, acc 0.609375
2020-02-09T01:23:34.360644: step 192, loss 0.962705, acc 0.578125
2020-02-09T01:23:34.548390: step 193, loss 0.965937, acc 0.640625
2020-02-09T01:23:34.732305: step 194, loss 0.875463, acc 0.609375
2020-02-09T01:23:34.932390: step 195, loss 0.889435, acc 0.59375
2020-02-09T01:23:35.139003: step 196, loss 0.770873, acc 0.609375
2020-02-09T01:23:35.341259: step 197, loss 1.02291, acc 0.546875
2020-02-09T01:23:35.562050: step 198, loss 0.754222, acc 0.671875
2020-02-09T01:23:35.746630: step 199, loss 0.794176, acc 0.609375
2020-02-09T01:23:35.951863: step 200, loss 0.83948, acc 0.578125

Evaluation:
2020-02-09T01:23:36.381200: step 200, loss 0.65621, acc 0.63227

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-200

2020-02-09T01:23:38.088078: step 201, loss 0.97769, acc 0.609375
2020-02-09T01:23:38.295072: step 202, loss 0.7388, acc 0.625
2020-02-09T01:23:38.506774: step 203, loss 0.746099, acc 0.625
2020-02-09T01:23:38.701992: step 204, loss 0.882438, acc 0.59375
2020-02-09T01:23:38.898265: step 205, loss 0.821059, acc 0.59375
2020-02-09T01:23:39.096649: step 206, loss 0.795215, acc 0.59375
2020-02-09T01:23:39.307119: step 207, loss 0.965858, acc 0.546875
2020-02-09T01:23:39.520088: step 208, loss 1.02355, acc 0.453125
2020-02-09T01:23:39.750068: step 209, loss 0.68779, acc 0.6875
2020-02-09T01:23:40.006468: step 210, loss 0.721944, acc 0.6875
2020-02-09T01:23:40.266570: step 211, loss 1.22715, acc 0.4375
2020-02-09T01:23:40.516600: step 212, loss 1.00621, acc 0.515625
2020-02-09T01:23:40.739582: step 213, loss 0.998371, acc 0.546875
2020-02-09T01:23:40.955693: step 214, loss 0.804432, acc 0.65625
2020-02-09T01:23:41.157596: step 215, loss 0.668423, acc 0.671875
2020-02-09T01:23:41.389016: step 216, loss 0.878059, acc 0.578125
2020-02-09T01:23:41.595237: step 217, loss 1.01059, acc 0.546875
2020-02-09T01:23:41.802397: step 218, loss 0.885317, acc 0.625
2020-02-09T01:23:42.009656: step 219, loss 0.855071, acc 0.6875
2020-02-09T01:23:42.221615: step 220, loss 0.743168, acc 0.625
2020-02-09T01:23:42.438009: step 221, loss 0.663051, acc 0.703125
2020-02-09T01:23:42.623875: step 222, loss 0.649027, acc 0.6875
2020-02-09T01:23:42.860219: step 223, loss 0.768153, acc 0.65625
2020-02-09T01:23:43.099297: step 224, loss 0.925839, acc 0.625
2020-02-09T01:23:43.301403: step 225, loss 0.976818, acc 0.515625
2020-02-09T01:23:43.526305: step 226, loss 0.905693, acc 0.53125
2020-02-09T01:23:43.770423: step 227, loss 0.523897, acc 0.8125
2020-02-09T01:23:44.009918: step 228, loss 0.800643, acc 0.671875
2020-02-09T01:23:44.243538: step 229, loss 0.700876, acc 0.6875
2020-02-09T01:23:44.480014: step 230, loss 0.772482, acc 0.640625
2020-02-09T01:23:44.709910: step 231, loss 0.73888, acc 0.65625
2020-02-09T01:23:45.014792: step 232, loss 0.767537, acc 0.59375
2020-02-09T01:23:45.258034: step 233, loss 0.770536, acc 0.5625
2020-02-09T01:23:45.480635: step 234, loss 0.823815, acc 0.515625
2020-02-09T01:23:45.727312: step 235, loss 0.643769, acc 0.65625
2020-02-09T01:23:45.939748: step 236, loss 0.887308, acc 0.546875
2020-02-09T01:23:46.135394: step 237, loss 1.20726, acc 0.484375
2020-02-09T01:23:46.353778: step 238, loss 0.944431, acc 0.578125
2020-02-09T01:23:46.578764: step 239, loss 0.968618, acc 0.578125
2020-02-09T01:23:46.767907: step 240, loss 0.782831, acc 0.65625
2020-02-09T01:23:46.987744: step 241, loss 0.783954, acc 0.640625
2020-02-09T01:23:47.200905: step 242, loss 0.938555, acc 0.59375
2020-02-09T01:23:47.376086: step 243, loss 0.977052, acc 0.59375
2020-02-09T01:23:47.589276: step 244, loss 0.877202, acc 0.578125
2020-02-09T01:23:47.794131: step 245, loss 0.756559, acc 0.609375
2020-02-09T01:23:48.005871: step 246, loss 0.853092, acc 0.578125
2020-02-09T01:23:48.226418: step 247, loss 0.846878, acc 0.53125
2020-02-09T01:23:48.435362: step 248, loss 0.873086, acc 0.53125
2020-02-09T01:23:48.656309: step 249, loss 0.861894, acc 0.59375
2020-02-09T01:23:48.853083: step 250, loss 0.711882, acc 0.65625
2020-02-09T01:23:49.072870: step 251, loss 0.64126, acc 0.703125
2020-02-09T01:23:49.241090: step 252, loss 0.858365, acc 0.578125
2020-02-09T01:23:49.454622: step 253, loss 0.785162, acc 0.625
2020-02-09T01:23:49.662841: step 254, loss 0.617364, acc 0.703125
2020-02-09T01:23:49.889812: step 255, loss 0.770024, acc 0.609375
2020-02-09T01:23:50.138346: step 256, loss 0.670263, acc 0.625
2020-02-09T01:23:50.364970: step 257, loss 0.935562, acc 0.53125
2020-02-09T01:23:50.609154: step 258, loss 0.633748, acc 0.703125
2020-02-09T01:23:50.818566: step 259, loss 0.709845, acc 0.609375
2020-02-09T01:23:50.990286: step 260, loss 0.757391, acc 0.671875
2020-02-09T01:23:51.211306: step 261, loss 1.01649, acc 0.546875
2020-02-09T01:23:51.374256: step 262, loss 0.759974, acc 0.578125
2020-02-09T01:23:51.576608: step 263, loss 0.787146, acc 0.671875
2020-02-09T01:23:51.794068: step 264, loss 0.677812, acc 0.65625
2020-02-09T01:23:52.019836: step 265, loss 0.650546, acc 0.75
2020-02-09T01:23:52.254493: step 266, loss 0.879435, acc 0.5625
2020-02-09T01:23:52.473524: step 267, loss 0.82054, acc 0.546875
2020-02-09T01:23:52.700282: step 268, loss 0.897375, acc 0.484375
2020-02-09T01:23:52.875644: step 269, loss 0.968029, acc 0.578125
2020-02-09T01:23:53.110911: step 270, loss 0.721191, acc 0.703125
2020-02-09T01:23:53.337535: step 271, loss 0.883396, acc 0.5625
2020-02-09T01:23:53.520485: step 272, loss 0.60193, acc 0.734375
2020-02-09T01:23:53.773281: step 273, loss 0.823349, acc 0.609375
2020-02-09T01:23:53.992018: step 274, loss 0.778253, acc 0.609375
2020-02-09T01:23:54.230853: step 275, loss 0.740666, acc 0.65625
2020-02-09T01:23:54.446786: step 276, loss 0.807678, acc 0.609375
2020-02-09T01:23:54.632533: step 277, loss 0.82406, acc 0.578125
2020-02-09T01:23:54.869362: step 278, loss 0.836021, acc 0.640625
2020-02-09T01:23:55.091170: step 279, loss 0.666648, acc 0.640625
2020-02-09T01:23:55.299985: step 280, loss 0.770474, acc 0.59375
2020-02-09T01:23:55.573294: step 281, loss 0.741162, acc 0.640625
2020-02-09T01:23:55.730312: step 282, loss 0.876837, acc 0.625
2020-02-09T01:23:55.939308: step 283, loss 0.766885, acc 0.609375
2020-02-09T01:23:56.158123: step 284, loss 0.697125, acc 0.609375
2020-02-09T01:23:56.395541: step 285, loss 0.773517, acc 0.625
2020-02-09T01:23:56.615672: step 286, loss 0.787353, acc 0.546875
2020-02-09T01:23:56.859366: step 287, loss 0.725129, acc 0.578125
2020-02-09T01:23:57.065087: step 288, loss 0.744094, acc 0.609375
2020-02-09T01:23:57.244231: step 289, loss 0.643628, acc 0.671875
2020-02-09T01:23:57.445443: step 290, loss 0.860179, acc 0.625
2020-02-09T01:23:57.674886: step 291, loss 0.72168, acc 0.640625
2020-02-09T01:23:57.897998: step 292, loss 0.883026, acc 0.5
2020-02-09T01:23:58.125323: step 293, loss 0.753292, acc 0.671875
2020-02-09T01:23:58.347310: step 294, loss 0.552746, acc 0.65625
2020-02-09T01:23:58.568848: step 295, loss 0.874644, acc 0.625
2020-02-09T01:23:58.803933: step 296, loss 0.744593, acc 0.640625
2020-02-09T01:23:59.026631: step 297, loss 0.654387, acc 0.65625
2020-02-09T01:23:59.250042: step 298, loss 0.701674, acc 0.625
2020-02-09T01:23:59.481399: step 299, loss 0.736459, acc 0.6875
2020-02-09T01:23:59.680939: step 300, loss 0.834415, acc 0.6

Evaluation:
2020-02-09T01:24:00.180683: step 300, loss 0.626669, acc 0.657598

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-300

2020-02-09T01:24:01.883288: step 301, loss 0.623834, acc 0.625
2020-02-09T01:24:02.076144: step 302, loss 0.607103, acc 0.734375
2020-02-09T01:24:02.246902: step 303, loss 0.773339, acc 0.59375
2020-02-09T01:24:02.454392: step 304, loss 0.666273, acc 0.671875
2020-02-09T01:24:02.660140: step 305, loss 0.936395, acc 0.546875
2020-02-09T01:24:02.859042: step 306, loss 0.699955, acc 0.71875
2020-02-09T01:24:03.052419: step 307, loss 0.702526, acc 0.59375
2020-02-09T01:24:03.259486: step 308, loss 0.689668, acc 0.609375
2020-02-09T01:24:03.463923: step 309, loss 0.678179, acc 0.640625
2020-02-09T01:24:03.632125: step 310, loss 0.872875, acc 0.546875
2020-02-09T01:24:03.816373: step 311, loss 0.75215, acc 0.578125
2020-02-09T01:24:04.002667: step 312, loss 0.602913, acc 0.671875
2020-02-09T01:24:04.209208: step 313, loss 0.714302, acc 0.671875
2020-02-09T01:24:04.401451: step 314, loss 0.713689, acc 0.59375
2020-02-09T01:24:04.596264: step 315, loss 0.584996, acc 0.640625
2020-02-09T01:24:04.787500: step 316, loss 0.64485, acc 0.625
2020-02-09T01:24:04.984252: step 317, loss 0.649991, acc 0.671875
2020-02-09T01:24:05.187932: step 318, loss 0.76677, acc 0.578125
2020-02-09T01:24:05.406627: step 319, loss 0.629489, acc 0.703125
2020-02-09T01:24:05.604585: step 320, loss 0.581197, acc 0.765625
2020-02-09T01:24:05.816286: step 321, loss 0.758401, acc 0.5
2020-02-09T01:24:05.995403: step 322, loss 0.577825, acc 0.703125
2020-02-09T01:24:06.204010: step 323, loss 0.64687, acc 0.71875
2020-02-09T01:24:06.400606: step 324, loss 0.551624, acc 0.71875
2020-02-09T01:24:06.586264: step 325, loss 0.577234, acc 0.703125
2020-02-09T01:24:06.781394: step 326, loss 0.666412, acc 0.6875
2020-02-09T01:24:06.977932: step 327, loss 0.58828, acc 0.671875
2020-02-09T01:24:07.172711: step 328, loss 0.675878, acc 0.703125
2020-02-09T01:24:07.368532: step 329, loss 0.694965, acc 0.640625
2020-02-09T01:24:07.585362: step 330, loss 0.564152, acc 0.71875
2020-02-09T01:24:07.768161: step 331, loss 0.485231, acc 0.796875
2020-02-09T01:24:07.964444: step 332, loss 0.662934, acc 0.6875
2020-02-09T01:24:08.160039: step 333, loss 0.839383, acc 0.53125
2020-02-09T01:24:08.356215: step 334, loss 0.605527, acc 0.671875
2020-02-09T01:24:08.552826: step 335, loss 0.654267, acc 0.671875
2020-02-09T01:24:08.745448: step 336, loss 0.650147, acc 0.65625
2020-02-09T01:24:08.938073: step 337, loss 0.576587, acc 0.671875
2020-02-09T01:24:09.130061: step 338, loss 0.633235, acc 0.671875
2020-02-09T01:24:09.331022: step 339, loss 0.654779, acc 0.703125
2020-02-09T01:24:09.517773: step 340, loss 0.608047, acc 0.65625
2020-02-09T01:24:09.736087: step 341, loss 0.649997, acc 0.734375
2020-02-09T01:24:09.962099: step 342, loss 0.657881, acc 0.65625
2020-02-09T01:24:10.150538: step 343, loss 0.515771, acc 0.765625
2020-02-09T01:24:10.354175: step 344, loss 0.660623, acc 0.65625
2020-02-09T01:24:10.608093: step 345, loss 0.62843, acc 0.65625
2020-02-09T01:24:10.856442: step 346, loss 0.609232, acc 0.609375
2020-02-09T01:24:11.038144: step 347, loss 0.535445, acc 0.734375
2020-02-09T01:24:11.258656: step 348, loss 0.743647, acc 0.703125
2020-02-09T01:24:11.504254: step 349, loss 0.53525, acc 0.65625
2020-02-09T01:24:11.722439: step 350, loss 0.589126, acc 0.625
2020-02-09T01:24:11.913211: step 351, loss 0.626874, acc 0.640625
2020-02-09T01:24:12.117288: step 352, loss 0.593372, acc 0.6875
2020-02-09T01:24:12.335664: step 353, loss 0.695415, acc 0.640625
2020-02-09T01:24:12.519005: step 354, loss 0.581015, acc 0.671875
2020-02-09T01:24:12.725941: step 355, loss 0.45049, acc 0.78125
2020-02-09T01:24:12.894274: step 356, loss 0.729261, acc 0.625
2020-02-09T01:24:13.092726: step 357, loss 0.637635, acc 0.734375
2020-02-09T01:24:13.304967: step 358, loss 0.630262, acc 0.703125
2020-02-09T01:24:13.502025: step 359, loss 0.745086, acc 0.59375
2020-02-09T01:24:13.702707: step 360, loss 0.656816, acc 0.65625
2020-02-09T01:24:13.902570: step 361, loss 0.518577, acc 0.65625
2020-02-09T01:24:14.099357: step 362, loss 0.61085, acc 0.65625
2020-02-09T01:24:14.300974: step 363, loss 0.580242, acc 0.734375
2020-02-09T01:24:14.508291: step 364, loss 0.703868, acc 0.6875
2020-02-09T01:24:14.704083: step 365, loss 0.535184, acc 0.71875
2020-02-09T01:24:14.897641: step 366, loss 0.631332, acc 0.65625
2020-02-09T01:24:15.094344: step 367, loss 0.601404, acc 0.703125
2020-02-09T01:24:15.290903: step 368, loss 0.556037, acc 0.71875
2020-02-09T01:24:15.499789: step 369, loss 0.624527, acc 0.703125
2020-02-09T01:24:15.737510: step 370, loss 0.618353, acc 0.703125
2020-02-09T01:24:15.955240: step 371, loss 0.717646, acc 0.6875
2020-02-09T01:24:16.169825: step 372, loss 0.571325, acc 0.734375
2020-02-09T01:24:16.391022: step 373, loss 0.621541, acc 0.640625
2020-02-09T01:24:16.591880: step 374, loss 0.745076, acc 0.59375
2020-02-09T01:24:16.798186: step 375, loss 0.6253, acc 0.625
2020-02-09T01:24:17.001191: step 376, loss 0.595066, acc 0.75
2020-02-09T01:24:17.208342: step 377, loss 0.667041, acc 0.71875
2020-02-09T01:24:17.420801: step 378, loss 0.680567, acc 0.578125
2020-02-09T01:24:17.651358: step 379, loss 0.680535, acc 0.609375
2020-02-09T01:24:17.845515: step 380, loss 0.572003, acc 0.75
2020-02-09T01:24:18.056208: step 381, loss 0.665153, acc 0.640625
2020-02-09T01:24:18.256000: step 382, loss 0.578465, acc 0.703125
2020-02-09T01:24:18.452897: step 383, loss 0.719478, acc 0.5625
2020-02-09T01:24:18.654703: step 384, loss 0.597345, acc 0.6875
2020-02-09T01:24:18.861692: step 385, loss 0.725173, acc 0.546875
2020-02-09T01:24:19.058053: step 386, loss 0.630972, acc 0.65625
2020-02-09T01:24:19.248743: step 387, loss 0.571748, acc 0.71875
2020-02-09T01:24:19.436109: step 388, loss 0.62451, acc 0.703125
2020-02-09T01:24:19.652928: step 389, loss 0.531003, acc 0.703125
2020-02-09T01:24:19.843270: step 390, loss 0.728693, acc 0.59375
2020-02-09T01:24:20.047461: step 391, loss 0.785819, acc 0.5
2020-02-09T01:24:20.243203: step 392, loss 0.76909, acc 0.59375
2020-02-09T01:24:20.475704: step 393, loss 0.600475, acc 0.6875
2020-02-09T01:24:20.642489: step 394, loss 0.755034, acc 0.609375
2020-02-09T01:24:20.852097: step 395, loss 0.719895, acc 0.609375
2020-02-09T01:24:21.085166: step 396, loss 0.618731, acc 0.640625
2020-02-09T01:24:21.313188: step 397, loss 0.619455, acc 0.6875
2020-02-09T01:24:21.524283: step 398, loss 0.458935, acc 0.8125
2020-02-09T01:24:21.739841: step 399, loss 0.645069, acc 0.625
2020-02-09T01:24:21.926054: step 400, loss 0.66107, acc 0.625

Evaluation:
2020-02-09T01:24:22.363653: step 400, loss 0.667669, acc 0.577861

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-400

2020-02-09T01:24:23.990934: step 401, loss 0.699794, acc 0.546875
2020-02-09T01:24:24.178196: step 402, loss 0.5537, acc 0.765625
2020-02-09T01:24:24.395478: step 403, loss 0.574712, acc 0.6875
2020-02-09T01:24:24.620751: step 404, loss 0.678751, acc 0.671875
2020-02-09T01:24:24.806080: step 405, loss 0.646562, acc 0.71875
2020-02-09T01:24:25.024729: step 406, loss 0.563287, acc 0.765625
2020-02-09T01:24:25.300607: step 407, loss 0.699457, acc 0.65625
2020-02-09T01:24:25.530285: step 408, loss 0.706583, acc 0.609375
2020-02-09T01:24:25.777156: step 409, loss 0.659132, acc 0.6875
2020-02-09T01:24:26.012774: step 410, loss 0.733123, acc 0.625
2020-02-09T01:24:26.248259: step 411, loss 0.687156, acc 0.640625
2020-02-09T01:24:26.488963: step 412, loss 0.612959, acc 0.671875
2020-02-09T01:24:26.671213: step 413, loss 0.539366, acc 0.71875
2020-02-09T01:24:26.921784: step 414, loss 0.794108, acc 0.5625
2020-02-09T01:24:27.149432: step 415, loss 0.621591, acc 0.6875
2020-02-09T01:24:27.372480: step 416, loss 0.779479, acc 0.546875
2020-02-09T01:24:27.613287: step 417, loss 0.617315, acc 0.671875
2020-02-09T01:24:27.838888: step 418, loss 0.682934, acc 0.65625
2020-02-09T01:24:28.053849: step 419, loss 0.621164, acc 0.703125
2020-02-09T01:24:28.286469: step 420, loss 0.589682, acc 0.71875
2020-02-09T01:24:28.519554: step 421, loss 0.606941, acc 0.640625
2020-02-09T01:24:28.742332: step 422, loss 0.601533, acc 0.671875
2020-02-09T01:24:28.917217: step 423, loss 0.610145, acc 0.625
2020-02-09T01:24:29.166583: step 424, loss 0.760672, acc 0.5625
2020-02-09T01:24:29.409315: step 425, loss 0.648554, acc 0.703125
2020-02-09T01:24:29.607864: step 426, loss 0.596755, acc 0.703125
2020-02-09T01:24:29.833116: step 427, loss 0.575224, acc 0.609375
2020-02-09T01:24:30.082442: step 428, loss 0.694055, acc 0.5625
2020-02-09T01:24:30.347327: step 429, loss 0.635008, acc 0.65625
2020-02-09T01:24:30.519484: step 430, loss 0.548983, acc 0.75
2020-02-09T01:24:30.743409: step 431, loss 0.62012, acc 0.65625
2020-02-09T01:24:30.959716: step 432, loss 0.606058, acc 0.671875
2020-02-09T01:24:31.218478: step 433, loss 0.652987, acc 0.65625
2020-02-09T01:24:31.390974: step 434, loss 0.740517, acc 0.5
2020-02-09T01:24:31.598360: step 435, loss 0.510932, acc 0.734375
2020-02-09T01:24:31.812424: step 436, loss 0.682026, acc 0.625
2020-02-09T01:24:32.021658: step 437, loss 0.56172, acc 0.734375
2020-02-09T01:24:32.233769: step 438, loss 0.722918, acc 0.609375
2020-02-09T01:24:32.413756: step 439, loss 0.564062, acc 0.734375
2020-02-09T01:24:32.611034: step 440, loss 0.553041, acc 0.765625
2020-02-09T01:24:32.827342: step 441, loss 0.398308, acc 0.875
2020-02-09T01:24:33.034927: step 442, loss 0.597832, acc 0.6875
2020-02-09T01:24:33.255812: step 443, loss 0.663576, acc 0.640625
2020-02-09T01:24:33.465357: step 444, loss 0.629275, acc 0.640625
2020-02-09T01:24:33.675936: step 445, loss 0.770237, acc 0.5625
2020-02-09T01:24:33.883520: step 446, loss 0.725946, acc 0.625
2020-02-09T01:24:34.084136: step 447, loss 0.663504, acc 0.671875
2020-02-09T01:24:34.294972: step 448, loss 0.724613, acc 0.546875
2020-02-09T01:24:34.497515: step 449, loss 0.680059, acc 0.5625
2020-02-09T01:24:34.684567: step 450, loss 0.716717, acc 0.633333
2020-02-09T01:24:34.913848: step 451, loss 0.644053, acc 0.671875
2020-02-09T01:24:35.141985: step 452, loss 0.617455, acc 0.65625
2020-02-09T01:24:35.362281: step 453, loss 0.573589, acc 0.6875
2020-02-09T01:24:35.572754: step 454, loss 0.612667, acc 0.6875
2020-02-09T01:24:35.808936: step 455, loss 0.636684, acc 0.6875
2020-02-09T01:24:36.052951: step 456, loss 0.66995, acc 0.609375
2020-02-09T01:24:36.229309: step 457, loss 0.701384, acc 0.59375
2020-02-09T01:24:36.414314: step 458, loss 0.591471, acc 0.734375
2020-02-09T01:24:36.670793: step 459, loss 0.535956, acc 0.6875
2020-02-09T01:24:36.884677: step 460, loss 0.600768, acc 0.671875
2020-02-09T01:24:37.015753: step 461, loss 0.645668, acc 0.640625
2020-02-09T01:24:37.146299: step 462, loss 0.593253, acc 0.734375
2020-02-09T01:24:37.277988: step 463, loss 0.545379, acc 0.734375
2020-02-09T01:24:37.418286: step 464, loss 0.496528, acc 0.796875
2020-02-09T01:24:37.547982: step 465, loss 0.479183, acc 0.796875
2020-02-09T01:24:37.678214: step 466, loss 0.581138, acc 0.75
2020-02-09T01:24:37.802691: step 467, loss 0.608906, acc 0.65625
2020-02-09T01:24:37.930938: step 468, loss 0.508885, acc 0.734375
2020-02-09T01:24:38.061432: step 469, loss 0.531436, acc 0.703125
2020-02-09T01:24:38.202026: step 470, loss 0.646497, acc 0.625
2020-02-09T01:24:38.347637: step 471, loss 0.494903, acc 0.765625
2020-02-09T01:24:38.598890: step 472, loss 0.510095, acc 0.703125
2020-02-09T01:24:38.796272: step 473, loss 0.568205, acc 0.734375
2020-02-09T01:24:38.996681: step 474, loss 0.563818, acc 0.71875
2020-02-09T01:24:39.189305: step 475, loss 0.773642, acc 0.5625
2020-02-09T01:24:39.412810: step 476, loss 0.492982, acc 0.75
2020-02-09T01:24:39.628929: step 477, loss 0.545268, acc 0.734375
2020-02-09T01:24:39.827932: step 478, loss 0.542571, acc 0.71875
2020-02-09T01:24:40.047476: step 479, loss 0.601305, acc 0.71875
2020-02-09T01:24:40.247258: step 480, loss 0.653606, acc 0.6875
2020-02-09T01:24:40.446104: step 481, loss 0.621574, acc 0.671875
2020-02-09T01:24:40.642706: step 482, loss 0.633604, acc 0.65625
2020-02-09T01:24:40.941354: step 483, loss 0.530847, acc 0.75
2020-02-09T01:24:41.157816: step 484, loss 0.569859, acc 0.671875
2020-02-09T01:24:41.352078: step 485, loss 0.715419, acc 0.671875
2020-02-09T01:24:41.548146: step 486, loss 0.544891, acc 0.6875
2020-02-09T01:24:41.751193: step 487, loss 0.48745, acc 0.78125
2020-02-09T01:24:42.048347: step 488, loss 0.652308, acc 0.59375
2020-02-09T01:24:42.269013: step 489, loss 0.593893, acc 0.65625
2020-02-09T01:24:42.494695: step 490, loss 0.618188, acc 0.71875
2020-02-09T01:24:42.705673: step 491, loss 0.541265, acc 0.671875
2020-02-09T01:24:42.997304: step 492, loss 0.636312, acc 0.578125
2020-02-09T01:24:43.169527: step 493, loss 0.593301, acc 0.703125
2020-02-09T01:24:43.304810: step 494, loss 0.59299, acc 0.71875
2020-02-09T01:24:43.439349: step 495, loss 0.569809, acc 0.765625
2020-02-09T01:24:43.569215: step 496, loss 0.637168, acc 0.71875
2020-02-09T01:24:43.698713: step 497, loss 0.566688, acc 0.640625
2020-02-09T01:24:43.830938: step 498, loss 0.49391, acc 0.734375
2020-02-09T01:24:43.964133: step 499, loss 0.436727, acc 0.796875
2020-02-09T01:24:44.165631: step 500, loss 0.602315, acc 0.71875

Evaluation:
2020-02-09T01:24:44.383397: step 500, loss 0.630872, acc 0.63227

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-500

2020-02-09T01:24:45.886478: step 501, loss 0.610897, acc 0.640625
2020-02-09T01:24:46.035876: step 502, loss 0.541325, acc 0.703125
2020-02-09T01:24:46.167547: step 503, loss 0.528835, acc 0.65625
2020-02-09T01:24:46.291161: step 504, loss 0.646095, acc 0.625
2020-02-09T01:24:46.416911: step 505, loss 0.509235, acc 0.6875
2020-02-09T01:24:46.546123: step 506, loss 0.598017, acc 0.640625
2020-02-09T01:24:46.678117: step 507, loss 0.561558, acc 0.671875
2020-02-09T01:24:46.800597: step 508, loss 0.564342, acc 0.703125
2020-02-09T01:24:46.926672: step 509, loss 0.511043, acc 0.765625
2020-02-09T01:24:47.049464: step 510, loss 0.626039, acc 0.734375
2020-02-09T01:24:47.175374: step 511, loss 0.512815, acc 0.75
2020-02-09T01:24:47.301197: step 512, loss 0.72174, acc 0.578125
2020-02-09T01:24:47.424938: step 513, loss 0.566731, acc 0.671875
2020-02-09T01:24:47.548749: step 514, loss 0.583287, acc 0.703125
2020-02-09T01:24:47.674726: step 515, loss 0.551048, acc 0.703125
2020-02-09T01:24:47.806940: step 516, loss 0.557229, acc 0.703125
2020-02-09T01:24:47.949285: step 517, loss 0.524384, acc 0.75
2020-02-09T01:24:48.075767: step 518, loss 0.482268, acc 0.75
2020-02-09T01:24:48.203639: step 519, loss 0.625176, acc 0.71875
2020-02-09T01:24:48.332333: step 520, loss 0.644634, acc 0.59375
2020-02-09T01:24:48.455511: step 521, loss 0.548036, acc 0.6875
2020-02-09T01:24:48.581808: step 522, loss 0.625253, acc 0.640625
2020-02-09T01:24:48.704800: step 523, loss 0.58575, acc 0.734375
2020-02-09T01:24:48.832929: step 524, loss 0.445806, acc 0.796875
2020-02-09T01:24:48.955662: step 525, loss 0.564987, acc 0.6875
2020-02-09T01:24:49.080730: step 526, loss 0.547981, acc 0.703125
2020-02-09T01:24:49.205496: step 527, loss 0.483864, acc 0.75
2020-02-09T01:24:49.334524: step 528, loss 0.564966, acc 0.75
2020-02-09T01:24:49.455735: step 529, loss 0.53974, acc 0.765625
2020-02-09T01:24:49.584116: step 530, loss 0.576844, acc 0.6875
2020-02-09T01:24:49.709199: step 531, loss 0.62001, acc 0.75
2020-02-09T01:24:49.836188: step 532, loss 0.678934, acc 0.71875
2020-02-09T01:24:49.990262: step 533, loss 0.524236, acc 0.671875
2020-02-09T01:24:50.114907: step 534, loss 0.538827, acc 0.6875
2020-02-09T01:24:50.248199: step 535, loss 0.623384, acc 0.640625
2020-02-09T01:24:50.375888: step 536, loss 0.59085, acc 0.6875
2020-02-09T01:24:50.500054: step 537, loss 0.579941, acc 0.734375
2020-02-09T01:24:50.624061: step 538, loss 0.570649, acc 0.734375
2020-02-09T01:24:50.754544: step 539, loss 0.608583, acc 0.671875
2020-02-09T01:24:50.881129: step 540, loss 0.540765, acc 0.765625
2020-02-09T01:24:51.004300: step 541, loss 0.514792, acc 0.796875
2020-02-09T01:24:51.131864: step 542, loss 0.612859, acc 0.6875
2020-02-09T01:24:51.256969: step 543, loss 0.722046, acc 0.671875
2020-02-09T01:24:51.382953: step 544, loss 0.556218, acc 0.734375
2020-02-09T01:24:51.504818: step 545, loss 0.588475, acc 0.640625
2020-02-09T01:24:51.633199: step 546, loss 0.670459, acc 0.578125
2020-02-09T01:24:51.759972: step 547, loss 0.606089, acc 0.6875
2020-02-09T01:24:51.889152: step 548, loss 0.610504, acc 0.71875
2020-02-09T01:24:52.016791: step 549, loss 0.482206, acc 0.75
2020-02-09T01:24:52.145157: step 550, loss 0.419078, acc 0.84375
2020-02-09T01:24:52.271393: step 551, loss 0.579804, acc 0.671875
2020-02-09T01:24:52.396037: step 552, loss 0.501579, acc 0.75
2020-02-09T01:24:52.518032: step 553, loss 0.529671, acc 0.671875
2020-02-09T01:24:52.643145: step 554, loss 0.456216, acc 0.84375
2020-02-09T01:24:52.771943: step 555, loss 0.458371, acc 0.796875
2020-02-09T01:24:52.900353: step 556, loss 0.569062, acc 0.78125
2020-02-09T01:24:53.025213: step 557, loss 0.569573, acc 0.703125
2020-02-09T01:24:53.150755: step 558, loss 0.530131, acc 0.671875
2020-02-09T01:24:53.276654: step 559, loss 0.516947, acc 0.71875
2020-02-09T01:24:53.399037: step 560, loss 0.462467, acc 0.78125
2020-02-09T01:24:53.518565: step 561, loss 0.554479, acc 0.671875
2020-02-09T01:24:53.640335: step 562, loss 0.533992, acc 0.71875
2020-02-09T01:24:53.762752: step 563, loss 0.610074, acc 0.65625
2020-02-09T01:24:53.925297: step 564, loss 0.618677, acc 0.71875
2020-02-09T01:24:54.198429: step 565, loss 0.640728, acc 0.65625
2020-02-09T01:24:54.452725: step 566, loss 0.534768, acc 0.734375
2020-02-09T01:24:54.669533: step 567, loss 0.685206, acc 0.609375
2020-02-09T01:24:54.857320: step 568, loss 0.471551, acc 0.765625
2020-02-09T01:24:54.984392: step 569, loss 0.605157, acc 0.6875
2020-02-09T01:24:55.106187: step 570, loss 0.633456, acc 0.6875
2020-02-09T01:24:55.271106: step 571, loss 0.594858, acc 0.6875
2020-02-09T01:24:55.550393: step 572, loss 0.513943, acc 0.78125
2020-02-09T01:24:55.797779: step 573, loss 0.54846, acc 0.703125
2020-02-09T01:24:55.944778: step 574, loss 0.566335, acc 0.703125
2020-02-09T01:24:56.171518: step 575, loss 0.472746, acc 0.71875
2020-02-09T01:24:56.470395: step 576, loss 0.513366, acc 0.71875
2020-02-09T01:24:56.660422: step 577, loss 0.604058, acc 0.703125
2020-02-09T01:24:56.918382: step 578, loss 0.511799, acc 0.703125
2020-02-09T01:24:57.180414: step 579, loss 0.507368, acc 0.765625
2020-02-09T01:24:57.409169: step 580, loss 0.576647, acc 0.75
2020-02-09T01:24:57.629000: step 581, loss 0.544868, acc 0.71875
2020-02-09T01:24:57.905692: step 582, loss 0.533279, acc 0.765625
2020-02-09T01:24:58.172921: step 583, loss 0.599035, acc 0.671875
2020-02-09T01:24:58.370542: step 584, loss 0.587021, acc 0.71875
2020-02-09T01:24:58.611297: step 585, loss 0.527455, acc 0.796875
2020-02-09T01:24:58.853701: step 586, loss 0.498504, acc 0.78125
2020-02-09T01:24:59.118723: step 587, loss 0.557032, acc 0.71875
2020-02-09T01:24:59.344475: step 588, loss 0.556145, acc 0.6875
2020-02-09T01:24:59.666934: step 589, loss 0.601725, acc 0.734375
2020-02-09T01:24:59.830678: step 590, loss 0.689977, acc 0.625
2020-02-09T01:25:00.056084: step 591, loss 0.50212, acc 0.796875
2020-02-09T01:25:00.206773: step 592, loss 0.574899, acc 0.6875
2020-02-09T01:25:00.440599: step 593, loss 0.550576, acc 0.65625
2020-02-09T01:25:00.656232: step 594, loss 0.616345, acc 0.65625
2020-02-09T01:25:00.886488: step 595, loss 0.515538, acc 0.765625
2020-02-09T01:25:01.175560: step 596, loss 0.591316, acc 0.75
2020-02-09T01:25:01.448636: step 597, loss 0.488237, acc 0.796875
2020-02-09T01:25:01.710858: step 598, loss 0.525897, acc 0.6875
2020-02-09T01:25:01.995881: step 599, loss 0.549556, acc 0.6875
2020-02-09T01:25:02.265049: step 600, loss 0.521128, acc 0.766667

Evaluation:
2020-02-09T01:25:02.795771: step 600, loss 0.652514, acc 0.606004

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-600

2020-02-09T01:25:04.559490: step 601, loss 0.517182, acc 0.78125
2020-02-09T01:25:04.715049: step 602, loss 0.500505, acc 0.796875
2020-02-09T01:25:04.904518: step 603, loss 0.525249, acc 0.734375
2020-02-09T01:25:05.128044: step 604, loss 0.501165, acc 0.75
2020-02-09T01:25:05.373519: step 605, loss 0.50952, acc 0.75
2020-02-09T01:25:05.617612: step 606, loss 0.480724, acc 0.734375
2020-02-09T01:25:05.879710: step 607, loss 0.516604, acc 0.71875
2020-02-09T01:25:06.173534: step 608, loss 0.70523, acc 0.640625
2020-02-09T01:25:06.351239: step 609, loss 0.564457, acc 0.734375
2020-02-09T01:25:06.624178: step 610, loss 0.532621, acc 0.8125
2020-02-09T01:25:06.873322: step 611, loss 0.447881, acc 0.8125
2020-02-09T01:25:07.162930: step 612, loss 0.467393, acc 0.765625
2020-02-09T01:25:07.307925: step 613, loss 0.455915, acc 0.78125
2020-02-09T01:25:07.449642: step 614, loss 0.635136, acc 0.640625
2020-02-09T01:25:07.625608: step 615, loss 0.554325, acc 0.6875
2020-02-09T01:25:07.881415: step 616, loss 0.554446, acc 0.703125
2020-02-09T01:25:08.135042: step 617, loss 0.578304, acc 0.640625
2020-02-09T01:25:08.388283: step 618, loss 0.397826, acc 0.8125
2020-02-09T01:25:08.632944: step 619, loss 0.477972, acc 0.6875
2020-02-09T01:25:08.901581: step 620, loss 0.555097, acc 0.734375
2020-02-09T01:25:09.152380: step 621, loss 0.601872, acc 0.734375
2020-02-09T01:25:09.402264: step 622, loss 0.575449, acc 0.71875
2020-02-09T01:25:09.653915: step 623, loss 0.466888, acc 0.78125
2020-02-09T01:25:09.932438: step 624, loss 0.429457, acc 0.765625
2020-02-09T01:25:10.217979: step 625, loss 0.433613, acc 0.796875
2020-02-09T01:25:10.503137: step 626, loss 0.560904, acc 0.671875
2020-02-09T01:25:10.770773: step 627, loss 0.410022, acc 0.8125
2020-02-09T01:25:10.963887: step 628, loss 0.500023, acc 0.734375
2020-02-09T01:25:11.263996: step 629, loss 0.563472, acc 0.734375
2020-02-09T01:25:11.528738: step 630, loss 0.507529, acc 0.71875
2020-02-09T01:25:11.760841: step 631, loss 0.570399, acc 0.734375
2020-02-09T01:25:12.012471: step 632, loss 0.492586, acc 0.765625
2020-02-09T01:25:12.269626: step 633, loss 0.438405, acc 0.78125
2020-02-09T01:25:12.539238: step 634, loss 0.601646, acc 0.65625
2020-02-09T01:25:12.728045: step 635, loss 0.588547, acc 0.71875
2020-02-09T01:25:12.994486: step 636, loss 0.524759, acc 0.71875
2020-02-09T01:25:13.243012: step 637, loss 0.400892, acc 0.8125
2020-02-09T01:25:13.451292: step 638, loss 0.392415, acc 0.859375
2020-02-09T01:25:13.658197: step 639, loss 0.50367, acc 0.734375
2020-02-09T01:25:13.944746: step 640, loss 0.493973, acc 0.75
2020-02-09T01:25:14.153982: step 641, loss 0.470491, acc 0.78125
2020-02-09T01:25:14.441246: step 642, loss 0.524727, acc 0.765625
2020-02-09T01:25:14.719484: step 643, loss 0.489696, acc 0.765625
2020-02-09T01:25:14.920223: step 644, loss 0.506948, acc 0.75
2020-02-09T01:25:15.151954: step 645, loss 0.575951, acc 0.671875
2020-02-09T01:25:15.394971: step 646, loss 0.473157, acc 0.796875
2020-02-09T01:25:15.604131: step 647, loss 0.69238, acc 0.609375
2020-02-09T01:25:15.827295: step 648, loss 0.426078, acc 0.828125
2020-02-09T01:25:16.046744: step 649, loss 0.389897, acc 0.8125
2020-02-09T01:25:16.231355: step 650, loss 0.389182, acc 0.859375
2020-02-09T01:25:16.442898: step 651, loss 0.490911, acc 0.75
2020-02-09T01:25:16.640348: step 652, loss 0.491356, acc 0.8125
2020-02-09T01:25:16.854020: step 653, loss 0.476732, acc 0.828125
2020-02-09T01:25:17.051478: step 654, loss 0.40737, acc 0.8125
2020-02-09T01:25:17.256748: step 655, loss 0.444508, acc 0.8125
2020-02-09T01:25:17.503892: step 656, loss 0.492576, acc 0.703125
2020-02-09T01:25:17.696247: step 657, loss 0.525666, acc 0.734375
2020-02-09T01:25:17.906366: step 658, loss 0.481325, acc 0.78125
2020-02-09T01:25:18.111447: step 659, loss 0.430906, acc 0.78125
2020-02-09T01:25:18.424634: step 660, loss 0.414643, acc 0.8125
2020-02-09T01:25:18.697490: step 661, loss 0.572781, acc 0.71875
2020-02-09T01:25:18.862649: step 662, loss 0.385534, acc 0.84375
2020-02-09T01:25:19.106049: step 663, loss 0.585345, acc 0.671875
2020-02-09T01:25:19.341248: step 664, loss 0.432049, acc 0.84375
2020-02-09T01:25:19.549260: step 665, loss 0.572559, acc 0.671875
2020-02-09T01:25:19.789588: step 666, loss 0.534582, acc 0.765625
2020-02-09T01:25:20.036382: step 667, loss 0.560225, acc 0.703125
2020-02-09T01:25:20.239432: step 668, loss 0.37945, acc 0.796875
2020-02-09T01:25:20.476289: step 669, loss 0.466168, acc 0.796875
2020-02-09T01:25:20.697320: step 670, loss 0.492594, acc 0.765625
2020-02-09T01:25:20.865327: step 671, loss 0.491914, acc 0.796875
2020-02-09T01:25:21.086021: step 672, loss 0.593605, acc 0.703125
2020-02-09T01:25:21.300919: step 673, loss 0.570309, acc 0.671875
2020-02-09T01:25:21.501850: step 674, loss 0.469952, acc 0.75
2020-02-09T01:25:21.712558: step 675, loss 0.458315, acc 0.78125
2020-02-09T01:25:21.926025: step 676, loss 0.452612, acc 0.828125
2020-02-09T01:25:22.137190: step 677, loss 0.413251, acc 0.84375
2020-02-09T01:25:22.345057: step 678, loss 0.576924, acc 0.71875
2020-02-09T01:25:22.540903: step 679, loss 0.477265, acc 0.765625
2020-02-09T01:25:22.734079: step 680, loss 0.582583, acc 0.6875
2020-02-09T01:25:22.929576: step 681, loss 0.459921, acc 0.75
2020-02-09T01:25:23.120777: step 682, loss 0.546672, acc 0.671875
2020-02-09T01:25:23.326327: step 683, loss 0.561664, acc 0.734375
2020-02-09T01:25:23.529325: step 684, loss 0.401952, acc 0.8125
2020-02-09T01:25:23.742870: step 685, loss 0.540292, acc 0.71875
2020-02-09T01:25:23.945188: step 686, loss 0.50482, acc 0.75
2020-02-09T01:25:24.196332: step 687, loss 0.491877, acc 0.75
2020-02-09T01:25:24.421862: step 688, loss 0.603869, acc 0.671875
2020-02-09T01:25:24.654617: step 689, loss 0.451802, acc 0.765625
2020-02-09T01:25:24.916894: step 690, loss 0.521861, acc 0.75
2020-02-09T01:25:25.180126: step 691, loss 0.396021, acc 0.796875
2020-02-09T01:25:25.433029: step 692, loss 0.616642, acc 0.71875
2020-02-09T01:25:25.678623: step 693, loss 0.555217, acc 0.6875
2020-02-09T01:25:26.056486: step 694, loss 0.439559, acc 0.8125
2020-02-09T01:25:26.277090: step 695, loss 0.492455, acc 0.71875
2020-02-09T01:25:26.547707: step 696, loss 0.602833, acc 0.734375
2020-02-09T01:25:26.815050: step 697, loss 0.462596, acc 0.75
2020-02-09T01:25:27.005153: step 698, loss 0.490242, acc 0.765625
2020-02-09T01:25:27.260180: step 699, loss 0.561464, acc 0.75
2020-02-09T01:25:27.495705: step 700, loss 0.444804, acc 0.78125

Evaluation:
2020-02-09T01:25:28.080308: step 700, loss 0.584833, acc 0.705441

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-700

2020-02-09T01:25:29.616205: step 701, loss 0.472349, acc 0.796875
2020-02-09T01:25:29.757116: step 702, loss 0.560826, acc 0.6875
2020-02-09T01:25:29.897438: step 703, loss 0.587279, acc 0.703125
2020-02-09T01:25:30.040393: step 704, loss 0.396458, acc 0.8125
2020-02-09T01:25:30.178520: step 705, loss 0.437405, acc 0.796875
2020-02-09T01:25:30.441678: step 706, loss 0.448844, acc 0.765625
2020-02-09T01:25:30.603151: step 707, loss 0.527141, acc 0.703125
2020-02-09T01:25:30.757850: step 708, loss 0.441959, acc 0.78125
2020-02-09T01:25:30.897370: step 709, loss 0.477188, acc 0.8125
2020-02-09T01:25:31.040805: step 710, loss 0.563713, acc 0.71875
2020-02-09T01:25:31.189596: step 711, loss 0.430621, acc 0.796875
2020-02-09T01:25:31.366772: step 712, loss 0.438518, acc 0.8125
2020-02-09T01:25:31.500517: step 713, loss 0.493064, acc 0.78125
2020-02-09T01:25:31.637122: step 714, loss 0.430731, acc 0.78125
2020-02-09T01:25:31.777621: step 715, loss 0.48003, acc 0.765625
2020-02-09T01:25:31.915217: step 716, loss 0.538872, acc 0.75
2020-02-09T01:25:32.049118: step 717, loss 0.511129, acc 0.6875
2020-02-09T01:25:32.222144: step 718, loss 0.48351, acc 0.71875
2020-02-09T01:25:32.358082: step 719, loss 0.353467, acc 0.859375
2020-02-09T01:25:32.492909: step 720, loss 0.55147, acc 0.734375
2020-02-09T01:25:32.633312: step 721, loss 0.604918, acc 0.75
2020-02-09T01:25:32.783975: step 722, loss 0.410552, acc 0.796875
2020-02-09T01:25:32.926339: step 723, loss 0.514699, acc 0.734375
2020-02-09T01:25:33.066427: step 724, loss 0.55373, acc 0.640625
2020-02-09T01:25:33.199313: step 725, loss 0.619021, acc 0.65625
2020-02-09T01:25:33.336337: step 726, loss 0.428445, acc 0.765625
2020-02-09T01:25:33.475289: step 727, loss 0.616983, acc 0.734375
2020-02-09T01:25:33.607734: step 728, loss 0.451417, acc 0.765625
2020-02-09T01:25:33.744445: step 729, loss 0.370534, acc 0.828125
2020-02-09T01:25:33.882926: step 730, loss 0.537949, acc 0.75
2020-02-09T01:25:34.043513: step 731, loss 0.617403, acc 0.671875
2020-02-09T01:25:34.211861: step 732, loss 0.615401, acc 0.703125
2020-02-09T01:25:34.380580: step 733, loss 0.366846, acc 0.84375
2020-02-09T01:25:34.534969: step 734, loss 0.514558, acc 0.734375
2020-02-09T01:25:34.671472: step 735, loss 0.464012, acc 0.75
2020-02-09T01:25:34.816485: step 736, loss 0.417267, acc 0.828125
2020-02-09T01:25:34.956177: step 737, loss 0.487524, acc 0.75
2020-02-09T01:25:35.115865: step 738, loss 0.439445, acc 0.828125
2020-02-09T01:25:35.307042: step 739, loss 0.393606, acc 0.78125
2020-02-09T01:25:35.455389: step 740, loss 0.45459, acc 0.78125
2020-02-09T01:25:35.613652: step 741, loss 0.585586, acc 0.75
2020-02-09T01:25:35.748745: step 742, loss 0.635533, acc 0.65625
2020-02-09T01:25:35.889671: step 743, loss 0.46422, acc 0.796875
2020-02-09T01:25:36.032463: step 744, loss 0.587969, acc 0.71875
2020-02-09T01:25:36.173805: step 745, loss 0.597915, acc 0.734375
2020-02-09T01:25:36.311846: step 746, loss 0.485332, acc 0.78125
2020-02-09T01:25:36.450825: step 747, loss 0.427419, acc 0.8125
2020-02-09T01:25:36.611986: step 748, loss 0.474628, acc 0.765625
2020-02-09T01:25:36.788937: step 749, loss 0.529961, acc 0.6875
2020-02-09T01:25:36.937367: step 750, loss 0.391175, acc 0.866667
2020-02-09T01:25:37.076147: step 751, loss 0.455587, acc 0.765625
2020-02-09T01:25:37.216221: step 752, loss 0.54173, acc 0.734375
2020-02-09T01:25:37.454500: step 753, loss 0.40316, acc 0.84375
2020-02-09T01:25:37.613778: step 754, loss 0.490303, acc 0.703125
2020-02-09T01:25:37.795721: step 755, loss 0.444279, acc 0.78125
2020-02-09T01:25:37.987787: step 756, loss 0.424874, acc 0.78125
2020-02-09T01:25:38.129627: step 757, loss 0.295266, acc 0.90625
2020-02-09T01:25:38.279333: step 758, loss 0.34851, acc 0.859375
2020-02-09T01:25:38.431890: step 759, loss 0.52448, acc 0.734375
2020-02-09T01:25:38.580025: step 760, loss 0.479059, acc 0.765625
2020-02-09T01:25:38.721053: step 761, loss 0.471643, acc 0.75
2020-02-09T01:25:38.853619: step 762, loss 0.420788, acc 0.828125
2020-02-09T01:25:38.984955: step 763, loss 0.483337, acc 0.78125
2020-02-09T01:25:39.121841: step 764, loss 0.398824, acc 0.890625
2020-02-09T01:25:39.257109: step 765, loss 0.419213, acc 0.84375
2020-02-09T01:25:39.395671: step 766, loss 0.356208, acc 0.90625
2020-02-09T01:25:39.532333: step 767, loss 0.448344, acc 0.796875
2020-02-09T01:25:39.662644: step 768, loss 0.407003, acc 0.8125
2020-02-09T01:25:39.792107: step 769, loss 0.376665, acc 0.859375
2020-02-09T01:25:39.932908: step 770, loss 0.434704, acc 0.796875
2020-02-09T01:25:40.067036: step 771, loss 0.357723, acc 0.828125
2020-02-09T01:25:40.199616: step 772, loss 0.419202, acc 0.859375
2020-02-09T01:25:40.339057: step 773, loss 0.449754, acc 0.8125
2020-02-09T01:25:40.472239: step 774, loss 0.33632, acc 0.828125
2020-02-09T01:25:40.605524: step 775, loss 0.546731, acc 0.734375
2020-02-09T01:25:40.741188: step 776, loss 0.374071, acc 0.84375
2020-02-09T01:25:40.872989: step 777, loss 0.387313, acc 0.828125
2020-02-09T01:25:41.004764: step 778, loss 0.370171, acc 0.8125
2020-02-09T01:25:41.149653: step 779, loss 0.447276, acc 0.75
2020-02-09T01:25:41.291600: step 780, loss 0.346483, acc 0.90625
2020-02-09T01:25:41.425435: step 781, loss 0.427932, acc 0.78125
2020-02-09T01:25:41.555121: step 782, loss 0.453392, acc 0.828125
2020-02-09T01:25:41.690333: step 783, loss 0.392499, acc 0.8125
2020-02-09T01:25:41.826363: step 784, loss 0.474577, acc 0.765625
2020-02-09T01:25:41.965255: step 785, loss 0.444692, acc 0.8125
2020-02-09T01:25:42.140452: step 786, loss 0.465719, acc 0.828125
2020-02-09T01:25:42.272445: step 787, loss 0.377852, acc 0.796875
2020-02-09T01:25:42.418281: step 788, loss 0.404551, acc 0.8125
2020-02-09T01:25:42.559305: step 789, loss 0.355471, acc 0.875
2020-02-09T01:25:42.724475: step 790, loss 0.449618, acc 0.78125
2020-02-09T01:25:42.921612: step 791, loss 0.42075, acc 0.84375
2020-02-09T01:25:43.058154: step 792, loss 0.520888, acc 0.671875
2020-02-09T01:25:43.206534: step 793, loss 0.453155, acc 0.828125
2020-02-09T01:25:43.358216: step 794, loss 0.413928, acc 0.796875
2020-02-09T01:25:43.535907: step 795, loss 0.458776, acc 0.828125
2020-02-09T01:25:43.692153: step 796, loss 0.395661, acc 0.8125
2020-02-09T01:25:43.823500: step 797, loss 0.444193, acc 0.796875
2020-02-09T01:25:43.957699: step 798, loss 0.502985, acc 0.78125
2020-02-09T01:25:44.094553: step 799, loss 0.446157, acc 0.84375
2020-02-09T01:25:44.224202: step 800, loss 0.352217, acc 0.84375

Evaluation:
2020-02-09T01:25:44.459866: step 800, loss 0.578734, acc 0.714822

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-800

2020-02-09T01:25:45.942061: step 801, loss 0.391978, acc 0.8125
2020-02-09T01:25:46.077292: step 802, loss 0.435001, acc 0.765625
2020-02-09T01:25:46.303779: step 803, loss 0.377521, acc 0.859375
2020-02-09T01:25:46.454152: step 804, loss 0.42233, acc 0.796875
2020-02-09T01:25:46.582710: step 805, loss 0.599836, acc 0.703125
2020-02-09T01:25:46.721309: step 806, loss 0.502587, acc 0.78125
2020-02-09T01:25:46.845176: step 807, loss 0.482421, acc 0.78125
2020-02-09T01:25:46.996456: step 808, loss 0.550635, acc 0.65625
2020-02-09T01:25:47.144664: step 809, loss 0.611276, acc 0.65625
2020-02-09T01:25:47.319927: step 810, loss 0.398543, acc 0.84375
2020-02-09T01:25:47.457839: step 811, loss 0.489304, acc 0.796875
2020-02-09T01:25:47.590265: step 812, loss 0.505035, acc 0.78125
2020-02-09T01:25:47.733386: step 813, loss 0.488363, acc 0.703125
2020-02-09T01:25:47.891753: step 814, loss 0.554756, acc 0.6875
2020-02-09T01:25:48.026551: step 815, loss 0.537885, acc 0.734375
2020-02-09T01:25:48.166816: step 816, loss 0.412387, acc 0.84375
2020-02-09T01:25:48.300141: step 817, loss 0.471753, acc 0.78125
2020-02-09T01:25:48.440739: step 818, loss 0.377808, acc 0.78125
2020-02-09T01:25:48.581594: step 819, loss 0.473757, acc 0.765625
2020-02-09T01:25:48.724970: step 820, loss 0.417037, acc 0.8125
2020-02-09T01:25:48.862052: step 821, loss 0.442324, acc 0.78125
2020-02-09T01:25:48.997949: step 822, loss 0.520714, acc 0.765625
2020-02-09T01:25:49.135491: step 823, loss 0.431713, acc 0.796875
2020-02-09T01:25:49.275192: step 824, loss 0.351661, acc 0.8125
2020-02-09T01:25:49.416253: step 825, loss 0.580789, acc 0.75
2020-02-09T01:25:49.550892: step 826, loss 0.261008, acc 0.921875
2020-02-09T01:25:49.695294: step 827, loss 0.497584, acc 0.75
2020-02-09T01:25:49.832288: step 828, loss 0.498337, acc 0.703125
2020-02-09T01:25:49.976598: step 829, loss 0.39595, acc 0.796875
2020-02-09T01:25:50.118323: step 830, loss 0.393295, acc 0.765625
2020-02-09T01:25:50.251386: step 831, loss 0.416109, acc 0.84375
2020-02-09T01:25:50.387985: step 832, loss 0.433812, acc 0.78125
2020-02-09T01:25:50.525437: step 833, loss 0.338154, acc 0.8125
2020-02-09T01:25:50.664209: step 834, loss 0.475992, acc 0.765625
2020-02-09T01:25:50.794743: step 835, loss 0.462667, acc 0.75
2020-02-09T01:25:50.937198: step 836, loss 0.351914, acc 0.828125
2020-02-09T01:25:51.071340: step 837, loss 0.389138, acc 0.8125
2020-02-09T01:25:51.207974: step 838, loss 0.455515, acc 0.765625
2020-02-09T01:25:51.364141: step 839, loss 0.248447, acc 0.9375
2020-02-09T01:25:51.498046: step 840, loss 0.568064, acc 0.6875
2020-02-09T01:25:51.642858: step 841, loss 0.482774, acc 0.796875
2020-02-09T01:25:51.776246: step 842, loss 0.453552, acc 0.734375
2020-02-09T01:25:51.921719: step 843, loss 0.438205, acc 0.8125
2020-02-09T01:25:52.055586: step 844, loss 0.358175, acc 0.859375
2020-02-09T01:25:52.194753: step 845, loss 0.557868, acc 0.78125
2020-02-09T01:25:52.338215: step 846, loss 0.426325, acc 0.859375
2020-02-09T01:25:52.523026: step 847, loss 0.418816, acc 0.796875
2020-02-09T01:25:52.662540: step 848, loss 0.363063, acc 0.8125
2020-02-09T01:25:52.800692: step 849, loss 0.31393, acc 0.890625
2020-02-09T01:25:52.961943: step 850, loss 0.368341, acc 0.828125
2020-02-09T01:25:53.096794: step 851, loss 0.445507, acc 0.8125
2020-02-09T01:25:53.236911: step 852, loss 0.36732, acc 0.859375
2020-02-09T01:25:53.375674: step 853, loss 0.492835, acc 0.75
2020-02-09T01:25:53.516743: step 854, loss 0.438607, acc 0.828125
2020-02-09T01:25:53.656356: step 855, loss 0.497129, acc 0.765625
2020-02-09T01:25:53.794005: step 856, loss 0.486696, acc 0.765625
2020-02-09T01:25:53.936741: step 857, loss 0.303213, acc 0.890625
2020-02-09T01:25:54.074646: step 858, loss 0.401823, acc 0.75
2020-02-09T01:25:54.213079: step 859, loss 0.391168, acc 0.875
2020-02-09T01:25:54.348008: step 860, loss 0.387488, acc 0.8125
2020-02-09T01:25:54.489409: step 861, loss 0.388312, acc 0.796875
2020-02-09T01:25:54.633842: step 862, loss 0.319315, acc 0.890625
2020-02-09T01:25:54.777075: step 863, loss 0.504741, acc 0.765625
2020-02-09T01:25:54.919251: step 864, loss 0.457744, acc 0.8125
2020-02-09T01:25:55.051032: step 865, loss 0.430689, acc 0.78125
2020-02-09T01:25:55.191162: step 866, loss 0.586179, acc 0.75
2020-02-09T01:25:55.329223: step 867, loss 0.556627, acc 0.71875
2020-02-09T01:25:55.469187: step 868, loss 0.53868, acc 0.765625
2020-02-09T01:25:55.599771: step 869, loss 0.471395, acc 0.734375
2020-02-09T01:25:55.743863: step 870, loss 0.483258, acc 0.78125
2020-02-09T01:25:55.881863: step 871, loss 0.510363, acc 0.71875
2020-02-09T01:25:56.023054: step 872, loss 0.481182, acc 0.78125
2020-02-09T01:25:56.159350: step 873, loss 0.520652, acc 0.765625
2020-02-09T01:25:56.301111: step 874, loss 0.448667, acc 0.75
2020-02-09T01:25:56.464174: step 875, loss 0.4684, acc 0.765625
2020-02-09T01:25:56.602654: step 876, loss 0.396734, acc 0.84375
2020-02-09T01:25:56.788937: step 877, loss 0.446102, acc 0.78125
2020-02-09T01:25:56.987580: step 878, loss 0.456078, acc 0.796875
2020-02-09T01:25:57.136390: step 879, loss 0.688008, acc 0.65625
2020-02-09T01:25:57.295014: step 880, loss 0.542092, acc 0.765625
2020-02-09T01:25:57.440276: step 881, loss 0.439427, acc 0.796875
2020-02-09T01:25:57.626203: step 882, loss 0.451994, acc 0.71875
2020-02-09T01:25:57.774608: step 883, loss 0.433596, acc 0.765625
2020-02-09T01:25:57.934136: step 884, loss 0.442453, acc 0.859375
2020-02-09T01:25:58.117581: step 885, loss 0.424516, acc 0.765625
2020-02-09T01:25:58.268178: step 886, loss 0.455826, acc 0.8125
2020-02-09T01:25:58.419584: step 887, loss 0.546928, acc 0.71875
2020-02-09T01:25:58.557762: step 888, loss 0.490274, acc 0.78125
2020-02-09T01:25:58.702195: step 889, loss 0.372465, acc 0.859375
2020-02-09T01:25:58.853668: step 890, loss 0.382941, acc 0.859375
2020-02-09T01:25:59.084491: step 891, loss 0.429687, acc 0.796875
2020-02-09T01:25:59.273411: step 892, loss 0.502473, acc 0.765625
2020-02-09T01:25:59.433168: step 893, loss 0.533821, acc 0.671875
2020-02-09T01:25:59.592061: step 894, loss 0.519593, acc 0.765625
2020-02-09T01:25:59.767251: step 895, loss 0.474694, acc 0.71875
2020-02-09T01:25:59.906015: step 896, loss 0.381358, acc 0.828125
2020-02-09T01:26:00.042025: step 897, loss 0.491747, acc 0.734375
2020-02-09T01:26:00.185029: step 898, loss 0.481745, acc 0.828125
2020-02-09T01:26:00.330407: step 899, loss 0.384394, acc 0.8125
2020-02-09T01:26:00.484030: step 900, loss 0.447284, acc 0.783333

Evaluation:
2020-02-09T01:26:00.718313: step 900, loss 0.571106, acc 0.723265

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-900

2020-02-09T01:26:02.260918: step 901, loss 0.400424, acc 0.828125
2020-02-09T01:26:02.392574: step 902, loss 0.373025, acc 0.78125
2020-02-09T01:26:02.537568: step 903, loss 0.329309, acc 0.828125
2020-02-09T01:26:02.673441: step 904, loss 0.316512, acc 0.875
2020-02-09T01:26:02.808017: step 905, loss 0.341403, acc 0.859375
2020-02-09T01:26:02.952758: step 906, loss 0.354042, acc 0.859375
2020-02-09T01:26:03.098447: step 907, loss 0.408609, acc 0.796875
2020-02-09T01:26:03.253328: step 908, loss 0.342378, acc 0.890625
2020-02-09T01:26:03.402304: step 909, loss 0.372737, acc 0.8125
2020-02-09T01:26:03.556836: step 910, loss 0.357873, acc 0.84375
2020-02-09T01:26:03.718781: step 911, loss 0.347198, acc 0.875
2020-02-09T01:26:03.862364: step 912, loss 0.39702, acc 0.8125
2020-02-09T01:26:04.021423: step 913, loss 0.243845, acc 0.90625
2020-02-09T01:26:04.192138: step 914, loss 0.370219, acc 0.8125
2020-02-09T01:26:04.354045: step 915, loss 0.36714, acc 0.828125
2020-02-09T01:26:04.522073: step 916, loss 0.415394, acc 0.796875
2020-02-09T01:26:04.702758: step 917, loss 0.337674, acc 0.859375
2020-02-09T01:26:04.933961: step 918, loss 0.226875, acc 0.9375
2020-02-09T01:26:05.097864: step 919, loss 0.31569, acc 0.875
2020-02-09T01:26:05.246113: step 920, loss 0.302501, acc 0.890625
2020-02-09T01:26:05.403347: step 921, loss 0.343093, acc 0.8125
2020-02-09T01:26:05.584740: step 922, loss 0.310499, acc 0.859375
2020-02-09T01:26:05.845422: step 923, loss 0.301727, acc 0.875
2020-02-09T01:26:06.022944: step 924, loss 0.362874, acc 0.84375
2020-02-09T01:26:06.179958: step 925, loss 0.262985, acc 0.90625
2020-02-09T01:26:06.300291: step 926, loss 0.335473, acc 0.828125
2020-02-09T01:26:06.450491: step 927, loss 0.43407, acc 0.796875
2020-02-09T01:26:06.604295: step 928, loss 0.312186, acc 0.859375
2020-02-09T01:26:06.734510: step 929, loss 0.288057, acc 0.90625
2020-02-09T01:26:06.907728: step 930, loss 0.437726, acc 0.78125
2020-02-09T01:26:07.042713: step 931, loss 0.359518, acc 0.859375
2020-02-09T01:26:07.174045: step 932, loss 0.397485, acc 0.8125
2020-02-09T01:26:07.304361: step 933, loss 0.263449, acc 0.875
2020-02-09T01:26:07.434903: step 934, loss 0.299432, acc 0.859375
2020-02-09T01:26:07.562921: step 935, loss 0.31868, acc 0.875
2020-02-09T01:26:07.697856: step 936, loss 0.351872, acc 0.875
2020-02-09T01:26:07.831967: step 937, loss 0.379109, acc 0.8125
2020-02-09T01:26:07.990715: step 938, loss 0.411275, acc 0.84375
2020-02-09T01:26:08.120250: step 939, loss 0.232639, acc 0.9375
2020-02-09T01:26:08.262484: step 940, loss 0.358508, acc 0.828125
2020-02-09T01:26:08.401749: step 941, loss 0.378568, acc 0.84375
2020-02-09T01:26:08.529698: step 942, loss 0.207672, acc 0.953125
2020-02-09T01:26:08.654103: step 943, loss 0.375332, acc 0.828125
2020-02-09T01:26:08.935295: step 944, loss 0.269677, acc 0.90625
2020-02-09T01:26:09.115051: step 945, loss 0.348553, acc 0.8125
2020-02-09T01:26:09.246497: step 946, loss 0.380076, acc 0.796875
2020-02-09T01:26:09.411279: step 947, loss 0.269923, acc 0.890625
2020-02-09T01:26:09.625898: step 948, loss 0.456035, acc 0.71875
2020-02-09T01:26:09.785810: step 949, loss 0.341916, acc 0.84375
2020-02-09T01:26:09.929316: step 950, loss 0.317463, acc 0.84375
2020-02-09T01:26:10.056180: step 951, loss 0.383108, acc 0.765625
2020-02-09T01:26:10.213107: step 952, loss 0.330432, acc 0.84375
2020-02-09T01:26:10.348240: step 953, loss 0.493021, acc 0.828125
2020-02-09T01:26:10.470455: step 954, loss 0.34769, acc 0.875
2020-02-09T01:26:10.591673: step 955, loss 0.379522, acc 0.84375
2020-02-09T01:26:10.711949: step 956, loss 0.352221, acc 0.828125
2020-02-09T01:26:10.846214: step 957, loss 0.403584, acc 0.78125
2020-02-09T01:26:10.979894: step 958, loss 0.233273, acc 0.90625
2020-02-09T01:26:11.101037: step 959, loss 0.504248, acc 0.75
2020-02-09T01:26:11.230099: step 960, loss 0.337451, acc 0.890625
2020-02-09T01:26:11.349526: step 961, loss 0.362998, acc 0.859375
2020-02-09T01:26:11.470349: step 962, loss 0.27318, acc 0.9375
2020-02-09T01:26:11.590428: step 963, loss 0.409874, acc 0.8125
2020-02-09T01:26:11.707437: step 964, loss 0.328378, acc 0.875
2020-02-09T01:26:11.824924: step 965, loss 0.364314, acc 0.875
2020-02-09T01:26:11.941522: step 966, loss 0.407629, acc 0.828125
2020-02-09T01:26:12.059445: step 967, loss 0.3378, acc 0.84375
2020-02-09T01:26:12.176145: step 968, loss 0.346712, acc 0.859375
2020-02-09T01:26:12.294076: step 969, loss 0.389047, acc 0.796875
2020-02-09T01:26:12.410656: step 970, loss 0.379295, acc 0.84375
2020-02-09T01:26:12.527978: step 971, loss 0.417206, acc 0.78125
2020-02-09T01:26:12.644096: step 972, loss 0.408771, acc 0.84375
2020-02-09T01:26:12.760648: step 973, loss 0.245668, acc 0.890625
2020-02-09T01:26:12.881604: step 974, loss 0.540174, acc 0.734375
2020-02-09T01:26:13.004410: step 975, loss 0.493602, acc 0.8125
2020-02-09T01:26:13.153958: step 976, loss 0.312416, acc 0.890625
2020-02-09T01:26:13.319771: step 977, loss 0.425079, acc 0.796875
2020-02-09T01:26:13.490498: step 978, loss 0.383482, acc 0.859375
2020-02-09T01:26:13.649428: step 979, loss 0.333453, acc 0.890625
2020-02-09T01:26:13.783153: step 980, loss 0.32315, acc 0.859375
2020-02-09T01:26:13.942794: step 981, loss 0.303156, acc 0.875
2020-02-09T01:26:14.102568: step 982, loss 0.365455, acc 0.859375
2020-02-09T01:26:14.363991: step 983, loss 0.27731, acc 0.890625
2020-02-09T01:26:14.490914: step 984, loss 0.367584, acc 0.84375
2020-02-09T01:26:14.662151: step 985, loss 0.395306, acc 0.796875
2020-02-09T01:26:14.778849: step 986, loss 0.394488, acc 0.828125
2020-02-09T01:26:14.900114: step 987, loss 0.388322, acc 0.84375
2020-02-09T01:26:15.050713: step 988, loss 0.323596, acc 0.84375
2020-02-09T01:26:15.185061: step 989, loss 0.285064, acc 0.890625
2020-02-09T01:26:15.347604: step 990, loss 0.383929, acc 0.84375
2020-02-09T01:26:15.519788: step 991, loss 0.330177, acc 0.890625
2020-02-09T01:26:15.656630: step 992, loss 0.411973, acc 0.78125
2020-02-09T01:26:15.782752: step 993, loss 0.295541, acc 0.890625
2020-02-09T01:26:15.902003: step 994, loss 0.341093, acc 0.859375
2020-02-09T01:26:16.056535: step 995, loss 0.38031, acc 0.828125
2020-02-09T01:26:16.195326: step 996, loss 0.397625, acc 0.859375
2020-02-09T01:26:16.322049: step 997, loss 0.295883, acc 0.859375
2020-02-09T01:26:16.444792: step 998, loss 0.418081, acc 0.796875
2020-02-09T01:26:16.564394: step 999, loss 0.372777, acc 0.8125
2020-02-09T01:26:16.725514: step 1000, loss 0.389561, acc 0.8125

Evaluation:
2020-02-09T01:26:16.968968: step 1000, loss 0.575955, acc 0.725141

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-1000

2020-02-09T01:26:18.466162: step 1001, loss 0.382609, acc 0.84375
2020-02-09T01:26:18.606198: step 1002, loss 0.441154, acc 0.796875
2020-02-09T01:26:18.736765: step 1003, loss 0.302746, acc 0.84375
2020-02-09T01:26:18.857081: step 1004, loss 0.268253, acc 0.890625
2020-02-09T01:26:18.975319: step 1005, loss 0.276048, acc 0.890625
2020-02-09T01:26:19.093010: step 1006, loss 0.444091, acc 0.734375
2020-02-09T01:26:19.216782: step 1007, loss 0.363714, acc 0.828125
2020-02-09T01:26:19.350426: step 1008, loss 0.499991, acc 0.734375
2020-02-09T01:26:19.489335: step 1009, loss 0.337276, acc 0.84375
2020-02-09T01:26:19.610748: step 1010, loss 0.354706, acc 0.875
2020-02-09T01:26:19.748707: step 1011, loss 0.375963, acc 0.828125
2020-02-09T01:26:19.887854: step 1012, loss 0.303153, acc 0.84375
2020-02-09T01:26:20.005659: step 1013, loss 0.420015, acc 0.828125
2020-02-09T01:26:20.126377: step 1014, loss 0.343039, acc 0.890625
2020-02-09T01:26:20.243339: step 1015, loss 0.390571, acc 0.8125
2020-02-09T01:26:20.357293: step 1016, loss 0.265246, acc 0.921875
2020-02-09T01:26:20.482716: step 1017, loss 0.240239, acc 0.9375
2020-02-09T01:26:20.600417: step 1018, loss 0.288641, acc 0.875
2020-02-09T01:26:20.721028: step 1019, loss 0.338989, acc 0.84375
2020-02-09T01:26:20.837161: step 1020, loss 0.371572, acc 0.828125
2020-02-09T01:26:20.953309: step 1021, loss 0.335277, acc 0.859375
2020-02-09T01:26:21.083357: step 1022, loss 0.401729, acc 0.796875
2020-02-09T01:26:21.218483: step 1023, loss 0.319496, acc 0.921875
2020-02-09T01:26:21.401102: step 1024, loss 0.380981, acc 0.8125
2020-02-09T01:26:21.533352: step 1025, loss 0.496059, acc 0.8125
2020-02-09T01:26:21.662093: step 1026, loss 0.507456, acc 0.78125
2020-02-09T01:26:21.793414: step 1027, loss 0.317905, acc 0.8125
2020-02-09T01:26:21.925941: step 1028, loss 0.418935, acc 0.796875
2020-02-09T01:26:22.058357: step 1029, loss 0.37686, acc 0.828125
2020-02-09T01:26:22.181824: step 1030, loss 0.401259, acc 0.78125
2020-02-09T01:26:22.298915: step 1031, loss 0.495499, acc 0.796875
2020-02-09T01:26:22.416593: step 1032, loss 0.330427, acc 0.8125
2020-02-09T01:26:22.534780: step 1033, loss 0.380197, acc 0.875
2020-02-09T01:26:22.649523: step 1034, loss 0.323174, acc 0.859375
2020-02-09T01:26:22.765040: step 1035, loss 0.351352, acc 0.828125
2020-02-09T01:26:22.882268: step 1036, loss 0.205463, acc 0.921875
2020-02-09T01:26:23.001632: step 1037, loss 0.414388, acc 0.78125
2020-02-09T01:26:23.120248: step 1038, loss 0.433068, acc 0.796875
2020-02-09T01:26:23.238382: step 1039, loss 0.43729, acc 0.78125
2020-02-09T01:26:23.355040: step 1040, loss 0.246549, acc 0.90625
2020-02-09T01:26:23.473528: step 1041, loss 0.350525, acc 0.84375
2020-02-09T01:26:23.595813: step 1042, loss 0.308991, acc 0.84375
2020-02-09T01:26:23.730663: step 1043, loss 0.272042, acc 0.875
2020-02-09T01:26:23.874400: step 1044, loss 0.459088, acc 0.78125
2020-02-09T01:26:24.002599: step 1045, loss 0.275641, acc 0.890625
2020-02-09T01:26:24.211759: step 1046, loss 0.270442, acc 0.890625
2020-02-09T01:26:24.342213: step 1047, loss 0.341396, acc 0.859375
2020-02-09T01:26:24.468818: step 1048, loss 0.321929, acc 0.875
2020-02-09T01:26:24.742695: step 1049, loss 0.348228, acc 0.875
2020-02-09T01:26:24.864743: step 1050, loss 0.373383, acc 0.816667
2020-02-09T01:26:24.987686: step 1051, loss 0.201177, acc 0.921875
2020-02-09T01:26:25.167119: step 1052, loss 0.196545, acc 0.921875
2020-02-09T01:26:25.286985: step 1053, loss 0.238954, acc 0.90625
2020-02-09T01:26:25.418597: step 1054, loss 0.349985, acc 0.828125
2020-02-09T01:26:25.608987: step 1055, loss 0.291403, acc 0.921875
2020-02-09T01:26:25.765878: step 1056, loss 0.373821, acc 0.84375
2020-02-09T01:26:25.920513: step 1057, loss 0.381918, acc 0.828125
2020-02-09T01:26:26.062170: step 1058, loss 0.243818, acc 0.875
2020-02-09T01:26:26.229949: step 1059, loss 0.242552, acc 0.921875
2020-02-09T01:26:26.386719: step 1060, loss 0.321382, acc 0.859375
2020-02-09T01:26:26.516068: step 1061, loss 0.277868, acc 0.890625
2020-02-09T01:26:26.679162: step 1062, loss 0.335259, acc 0.875
2020-02-09T01:26:26.821312: step 1063, loss 0.339836, acc 0.796875
2020-02-09T01:26:26.964795: step 1064, loss 0.318533, acc 0.875
2020-02-09T01:26:27.097897: step 1065, loss 0.170011, acc 0.953125
2020-02-09T01:26:27.265152: step 1066, loss 0.236727, acc 0.90625
2020-02-09T01:26:27.398013: step 1067, loss 0.232644, acc 0.890625
2020-02-09T01:26:27.531651: step 1068, loss 0.330579, acc 0.828125
2020-02-09T01:26:27.669711: step 1069, loss 0.245269, acc 0.921875
2020-02-09T01:26:27.787204: step 1070, loss 0.20493, acc 0.921875
2020-02-09T01:26:27.916464: step 1071, loss 0.320247, acc 0.875
2020-02-09T01:26:28.043974: step 1072, loss 0.364942, acc 0.84375
2020-02-09T01:26:28.164454: step 1073, loss 0.302993, acc 0.921875
2020-02-09T01:26:28.282946: step 1074, loss 0.251037, acc 0.890625
2020-02-09T01:26:28.406108: step 1075, loss 0.245081, acc 0.859375
2020-02-09T01:26:28.544338: step 1076, loss 0.323328, acc 0.859375
2020-02-09T01:26:28.671733: step 1077, loss 0.213949, acc 0.90625
2020-02-09T01:26:28.792931: step 1078, loss 0.275079, acc 0.875
2020-02-09T01:26:28.933238: step 1079, loss 0.231998, acc 0.9375
2020-02-09T01:26:29.074588: step 1080, loss 0.338289, acc 0.84375
2020-02-09T01:26:29.218000: step 1081, loss 0.427052, acc 0.828125
2020-02-09T01:26:29.341479: step 1082, loss 0.262544, acc 0.875
2020-02-09T01:26:29.467428: step 1083, loss 0.333998, acc 0.84375
2020-02-09T01:26:29.592775: step 1084, loss 0.303954, acc 0.875
2020-02-09T01:26:29.745329: step 1085, loss 0.322761, acc 0.875
2020-02-09T01:26:29.883772: step 1086, loss 0.26461, acc 0.875
2020-02-09T01:26:30.015531: step 1087, loss 0.267466, acc 0.875
2020-02-09T01:26:30.145072: step 1088, loss 0.266287, acc 0.859375
2020-02-09T01:26:30.278768: step 1089, loss 0.2429, acc 0.921875
2020-02-09T01:26:30.404648: step 1090, loss 0.251569, acc 0.921875
2020-02-09T01:26:30.528475: step 1091, loss 0.231942, acc 0.875
2020-02-09T01:26:30.655421: step 1092, loss 0.325482, acc 0.890625
2020-02-09T01:26:30.776627: step 1093, loss 0.219143, acc 0.90625
2020-02-09T01:26:30.893315: step 1094, loss 0.282189, acc 0.90625
2020-02-09T01:26:31.016162: step 1095, loss 0.165209, acc 0.921875
2020-02-09T01:26:31.138576: step 1096, loss 0.232046, acc 0.921875
2020-02-09T01:26:31.266381: step 1097, loss 0.261719, acc 0.890625
2020-02-09T01:26:31.391458: step 1098, loss 0.196889, acc 0.9375
2020-02-09T01:26:31.515204: step 1099, loss 0.353312, acc 0.84375
2020-02-09T01:26:31.659116: step 1100, loss 0.263066, acc 0.890625

Evaluation:
2020-02-09T01:26:31.872762: step 1100, loss 0.57774, acc 0.71576

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-1100

2020-02-09T01:26:33.370475: step 1101, loss 0.188722, acc 0.9375
2020-02-09T01:26:33.498707: step 1102, loss 0.284007, acc 0.859375
2020-02-09T01:26:33.640037: step 1103, loss 0.263868, acc 0.921875
2020-02-09T01:26:33.757994: step 1104, loss 0.336734, acc 0.890625
2020-02-09T01:26:33.878342: step 1105, loss 0.226884, acc 0.90625
2020-02-09T01:26:34.005648: step 1106, loss 0.24948, acc 0.96875
2020-02-09T01:26:34.129276: step 1107, loss 0.353659, acc 0.84375
2020-02-09T01:26:34.259789: step 1108, loss 0.391098, acc 0.796875
2020-02-09T01:26:34.382233: step 1109, loss 0.252012, acc 0.859375
2020-02-09T01:26:34.508269: step 1110, loss 0.265527, acc 0.875
2020-02-09T01:26:34.630199: step 1111, loss 0.236317, acc 0.953125
2020-02-09T01:26:34.752866: step 1112, loss 0.248383, acc 0.90625
2020-02-09T01:26:34.875603: step 1113, loss 0.251398, acc 0.890625
2020-02-09T01:26:34.996386: step 1114, loss 0.276666, acc 0.890625
2020-02-09T01:26:35.122771: step 1115, loss 0.213403, acc 0.9375
2020-02-09T01:26:35.250141: step 1116, loss 0.309319, acc 0.890625
2020-02-09T01:26:35.367492: step 1117, loss 0.351956, acc 0.8125
2020-02-09T01:26:35.485683: step 1118, loss 0.227935, acc 0.921875
2020-02-09T01:26:35.601929: step 1119, loss 0.177823, acc 0.921875
2020-02-09T01:26:35.722192: step 1120, loss 0.260362, acc 0.921875
2020-02-09T01:26:35.840564: step 1121, loss 0.269889, acc 0.859375
2020-02-09T01:26:35.959556: step 1122, loss 0.245517, acc 0.90625
2020-02-09T01:26:36.082368: step 1123, loss 0.351824, acc 0.828125
2020-02-09T01:26:36.200786: step 1124, loss 0.390966, acc 0.84375
2020-02-09T01:26:36.323933: step 1125, loss 0.215832, acc 0.90625
2020-02-09T01:26:36.460158: step 1126, loss 0.371127, acc 0.859375
2020-02-09T01:26:36.603013: step 1127, loss 0.240131, acc 0.921875
2020-02-09T01:26:36.731343: step 1128, loss 0.266796, acc 0.875
2020-02-09T01:26:36.859340: step 1129, loss 0.304588, acc 0.875
2020-02-09T01:26:36.981491: step 1130, loss 0.323441, acc 0.890625
2020-02-09T01:26:37.104919: step 1131, loss 0.295499, acc 0.890625
2020-02-09T01:26:37.230613: step 1132, loss 0.234666, acc 0.890625
2020-02-09T01:26:37.351119: step 1133, loss 0.251409, acc 0.921875
2020-02-09T01:26:37.473538: step 1134, loss 0.324962, acc 0.875
2020-02-09T01:26:37.592402: step 1135, loss 0.188877, acc 0.9375
2020-02-09T01:26:37.723505: step 1136, loss 0.314937, acc 0.828125
2020-02-09T01:26:37.868031: step 1137, loss 0.193883, acc 0.9375
2020-02-09T01:26:37.991591: step 1138, loss 0.265345, acc 0.84375
2020-02-09T01:26:38.126815: step 1139, loss 0.285914, acc 0.90625
2020-02-09T01:26:38.250661: step 1140, loss 0.278808, acc 0.84375
2020-02-09T01:26:38.378110: step 1141, loss 0.256739, acc 0.890625
2020-02-09T01:26:38.501731: step 1142, loss 0.364838, acc 0.90625
2020-02-09T01:26:38.627474: step 1143, loss 0.301385, acc 0.875
2020-02-09T01:26:38.755806: step 1144, loss 0.414202, acc 0.796875
2020-02-09T01:26:38.873655: step 1145, loss 0.307698, acc 0.890625
2020-02-09T01:26:38.993656: step 1146, loss 0.229458, acc 0.90625
2020-02-09T01:26:39.112136: step 1147, loss 0.301429, acc 0.890625
2020-02-09T01:26:39.230983: step 1148, loss 0.38472, acc 0.78125
2020-02-09T01:26:39.355893: step 1149, loss 0.284445, acc 0.875
2020-02-09T01:26:39.516072: step 1150, loss 0.301848, acc 0.890625
2020-02-09T01:26:39.656642: step 1151, loss 0.284211, acc 0.84375
2020-02-09T01:26:39.773745: step 1152, loss 0.400255, acc 0.78125
2020-02-09T01:26:39.891622: step 1153, loss 0.303997, acc 0.84375
2020-02-09T01:26:40.007857: step 1154, loss 0.278667, acc 0.90625
2020-02-09T01:26:40.130750: step 1155, loss 0.371529, acc 0.8125
2020-02-09T01:26:40.265326: step 1156, loss 0.396854, acc 0.828125
2020-02-09T01:26:40.392275: step 1157, loss 0.306548, acc 0.828125
2020-02-09T01:26:40.520571: step 1158, loss 0.355771, acc 0.859375
2020-02-09T01:26:40.642393: step 1159, loss 0.217505, acc 0.921875
2020-02-09T01:26:40.763385: step 1160, loss 0.357125, acc 0.8125
2020-02-09T01:26:40.899040: step 1161, loss 0.229428, acc 0.921875
2020-02-09T01:26:41.021454: step 1162, loss 0.2975, acc 0.890625
2020-02-09T01:26:41.146237: step 1163, loss 0.219161, acc 0.90625
2020-02-09T01:26:41.299344: step 1164, loss 0.452014, acc 0.796875
2020-02-09T01:26:41.426318: step 1165, loss 0.40918, acc 0.8125
2020-02-09T01:26:41.552304: step 1166, loss 0.311446, acc 0.828125
2020-02-09T01:26:41.686088: step 1167, loss 0.295571, acc 0.9375
2020-02-09T01:26:41.840924: step 1168, loss 0.290162, acc 0.859375
2020-02-09T01:26:41.990866: step 1169, loss 0.306358, acc 0.890625
2020-02-09T01:26:42.109162: step 1170, loss 0.26564, acc 0.859375
2020-02-09T01:26:42.241359: step 1171, loss 0.358562, acc 0.859375
2020-02-09T01:26:42.370595: step 1172, loss 0.31517, acc 0.875
2020-02-09T01:26:42.537426: step 1173, loss 0.32672, acc 0.859375
2020-02-09T01:26:42.717829: step 1174, loss 0.299999, acc 0.875
2020-02-09T01:26:42.874664: step 1175, loss 0.209988, acc 0.921875
2020-02-09T01:26:43.020343: step 1176, loss 0.282994, acc 0.890625
2020-02-09T01:26:43.169952: step 1177, loss 0.30464, acc 0.890625
2020-02-09T01:26:43.326072: step 1178, loss 0.322064, acc 0.90625
2020-02-09T01:26:43.455935: step 1179, loss 0.295594, acc 0.875
2020-02-09T01:26:43.584936: step 1180, loss 0.254528, acc 0.90625
2020-02-09T01:26:43.716134: step 1181, loss 0.357766, acc 0.84375
2020-02-09T01:26:43.858967: step 1182, loss 0.317052, acc 0.84375
2020-02-09T01:26:44.050070: step 1183, loss 0.217429, acc 0.890625
2020-02-09T01:26:44.801431: step 1184, loss 0.268085, acc 0.890625
2020-02-09T01:26:44.938701: step 1185, loss 0.296123, acc 0.90625
2020-02-09T01:26:45.075474: step 1186, loss 0.273449, acc 0.890625
2020-02-09T01:26:45.201245: step 1187, loss 0.406294, acc 0.875
2020-02-09T01:26:45.327337: step 1188, loss 0.233451, acc 0.921875
2020-02-09T01:26:45.465756: step 1189, loss 0.338791, acc 0.84375
2020-02-09T01:26:45.615365: step 1190, loss 0.268198, acc 0.875
2020-02-09T01:26:45.748591: step 1191, loss 0.253479, acc 0.875
2020-02-09T01:26:45.924636: step 1192, loss 0.441869, acc 0.8125
2020-02-09T01:26:46.072430: step 1193, loss 0.276845, acc 0.921875
2020-02-09T01:26:46.205844: step 1194, loss 0.446028, acc 0.765625
2020-02-09T01:26:46.338561: step 1195, loss 0.373472, acc 0.828125
2020-02-09T01:26:46.474184: step 1196, loss 0.424429, acc 0.8125
2020-02-09T01:26:46.606839: step 1197, loss 0.251593, acc 0.890625
2020-02-09T01:26:46.731352: step 1198, loss 0.327551, acc 0.84375
2020-02-09T01:26:46.859522: step 1199, loss 0.398191, acc 0.84375
2020-02-09T01:26:46.978682: step 1200, loss 0.331442, acc 0.833333

Evaluation:
2020-02-09T01:26:47.196858: step 1200, loss 0.565505, acc 0.737336

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-1200

2020-02-09T01:26:48.905386: step 1201, loss 0.2283, acc 0.953125
2020-02-09T01:26:49.039382: step 1202, loss 0.262438, acc 0.90625
2020-02-09T01:26:49.186513: step 1203, loss 0.184852, acc 0.921875
2020-02-09T01:26:49.321575: step 1204, loss 0.155941, acc 0.9375
2020-02-09T01:26:49.490048: step 1205, loss 0.25832, acc 0.859375
2020-02-09T01:26:49.637024: step 1206, loss 0.244572, acc 0.90625
2020-02-09T01:26:49.781018: step 1207, loss 0.249924, acc 0.90625
2020-02-09T01:26:49.924402: step 1208, loss 0.327857, acc 0.875
2020-02-09T01:26:50.061601: step 1209, loss 0.243135, acc 0.890625
2020-02-09T01:26:50.211967: step 1210, loss 0.281817, acc 0.875
2020-02-09T01:26:50.375055: step 1211, loss 0.330579, acc 0.890625
2020-02-09T01:26:50.511591: step 1212, loss 0.202841, acc 0.90625
2020-02-09T01:26:50.652666: step 1213, loss 0.14955, acc 0.953125
2020-02-09T01:26:50.787163: step 1214, loss 0.251245, acc 0.890625
2020-02-09T01:26:50.925544: step 1215, loss 0.136048, acc 0.96875
2020-02-09T01:26:51.072206: step 1216, loss 0.320969, acc 0.828125
2020-02-09T01:26:51.213990: step 1217, loss 0.328247, acc 0.875
2020-02-09T01:26:51.347642: step 1218, loss 0.350151, acc 0.875
2020-02-09T01:26:51.480713: step 1219, loss 0.247337, acc 0.921875
2020-02-09T01:26:51.615737: step 1220, loss 0.304035, acc 0.859375
2020-02-09T01:26:51.748742: step 1221, loss 0.139658, acc 0.953125
2020-02-09T01:26:51.940830: step 1222, loss 0.191011, acc 0.90625
2020-02-09T01:26:52.108429: step 1223, loss 0.233448, acc 0.890625
2020-02-09T01:26:52.244206: step 1224, loss 0.184626, acc 0.953125
2020-02-09T01:26:52.393727: step 1225, loss 0.15581, acc 0.9375
2020-02-09T01:26:52.533786: step 1226, loss 0.157714, acc 0.96875
2020-02-09T01:26:52.667283: step 1227, loss 0.173513, acc 0.921875
2020-02-09T01:26:52.815015: step 1228, loss 0.338939, acc 0.828125
2020-02-09T01:26:52.971699: step 1229, loss 0.340429, acc 0.875
2020-02-09T01:26:53.117420: step 1230, loss 0.134851, acc 0.96875
2020-02-09T01:26:53.304064: step 1231, loss 0.27499, acc 0.859375
2020-02-09T01:26:53.479489: step 1232, loss 0.150675, acc 0.96875
2020-02-09T01:26:53.623408: step 1233, loss 0.201076, acc 0.890625
2020-02-09T01:26:53.761496: step 1234, loss 0.224796, acc 0.875
2020-02-09T01:26:53.894629: step 1235, loss 0.30016, acc 0.875
2020-02-09T01:26:54.030313: step 1236, loss 0.145476, acc 0.9375
2020-02-09T01:26:54.170119: step 1237, loss 0.243587, acc 0.875
2020-02-09T01:26:54.302937: step 1238, loss 0.148856, acc 0.96875
2020-02-09T01:26:54.447885: step 1239, loss 0.160564, acc 0.9375
2020-02-09T01:26:54.592009: step 1240, loss 0.237598, acc 0.921875
2020-02-09T01:26:54.733129: step 1241, loss 0.19089, acc 0.921875
2020-02-09T01:26:54.871397: step 1242, loss 0.138065, acc 0.96875
2020-02-09T01:26:55.015788: step 1243, loss 0.183587, acc 0.9375
2020-02-09T01:26:55.158770: step 1244, loss 0.236574, acc 0.921875
2020-02-09T01:26:55.301807: step 1245, loss 0.294681, acc 0.875
2020-02-09T01:26:55.459629: step 1246, loss 0.231013, acc 0.890625
2020-02-09T01:26:55.604145: step 1247, loss 0.212338, acc 0.9375
2020-02-09T01:26:55.742957: step 1248, loss 0.225341, acc 0.875
2020-02-09T01:26:55.889096: step 1249, loss 0.139526, acc 0.953125
2020-02-09T01:26:56.034933: step 1250, loss 0.219974, acc 0.890625
2020-02-09T01:26:56.187853: step 1251, loss 0.195103, acc 0.90625
2020-02-09T01:26:56.335748: step 1252, loss 0.250901, acc 0.9375
2020-02-09T01:26:56.478257: step 1253, loss 0.233146, acc 0.90625
2020-02-09T01:26:56.631284: step 1254, loss 0.287978, acc 0.859375
2020-02-09T01:26:56.793561: step 1255, loss 0.196237, acc 0.921875
2020-02-09T01:26:56.932574: step 1256, loss 0.20219, acc 0.921875
2020-02-09T01:26:57.076099: step 1257, loss 0.249414, acc 0.921875
2020-02-09T01:26:57.219448: step 1258, loss 0.230637, acc 0.921875
2020-02-09T01:26:57.358860: step 1259, loss 0.307897, acc 0.875
2020-02-09T01:26:57.596530: step 1260, loss 0.222636, acc 0.890625
2020-02-09T01:26:57.758047: step 1261, loss 0.236271, acc 0.90625
2020-02-09T01:26:57.895034: step 1262, loss 0.137583, acc 0.953125
2020-02-09T01:26:58.035324: step 1263, loss 0.156673, acc 0.96875
2020-02-09T01:26:58.168290: step 1264, loss 0.271001, acc 0.875
2020-02-09T01:26:58.309836: step 1265, loss 0.229638, acc 0.875
2020-02-09T01:26:58.461926: step 1266, loss 0.250899, acc 0.875
2020-02-09T01:26:58.626555: step 1267, loss 0.296227, acc 0.890625
2020-02-09T01:26:58.760985: step 1268, loss 0.196639, acc 0.90625
2020-02-09T01:26:58.900724: step 1269, loss 0.242411, acc 0.875
2020-02-09T01:26:59.054233: step 1270, loss 0.0937782, acc 1
2020-02-09T01:26:59.216637: step 1271, loss 0.151489, acc 0.96875
2020-02-09T01:26:59.360974: step 1272, loss 0.248125, acc 0.859375
2020-02-09T01:26:59.515150: step 1273, loss 0.252717, acc 0.90625
2020-02-09T01:26:59.650722: step 1274, loss 0.228093, acc 0.890625
2020-02-09T01:26:59.784742: step 1275, loss 0.188335, acc 0.921875
2020-02-09T01:26:59.920158: step 1276, loss 0.205341, acc 0.90625
2020-02-09T01:27:00.057520: step 1277, loss 0.123372, acc 0.984375
2020-02-09T01:27:00.194324: step 1278, loss 0.188059, acc 0.9375
2020-02-09T01:27:00.331281: step 1279, loss 0.246564, acc 0.90625
2020-02-09T01:27:00.452910: step 1280, loss 0.247767, acc 0.859375
2020-02-09T01:27:00.578171: step 1281, loss 0.332429, acc 0.859375
2020-02-09T01:27:00.696290: step 1282, loss 0.244199, acc 0.890625
2020-02-09T01:27:00.819273: step 1283, loss 0.272114, acc 0.890625
2020-02-09T01:27:00.939520: step 1284, loss 0.292633, acc 0.796875
2020-02-09T01:27:01.057463: step 1285, loss 0.209037, acc 0.953125
2020-02-09T01:27:01.182823: step 1286, loss 0.29807, acc 0.875
2020-02-09T01:27:01.302721: step 1287, loss 0.342833, acc 0.859375
2020-02-09T01:27:01.423254: step 1288, loss 0.28788, acc 0.875
2020-02-09T01:27:01.546876: step 1289, loss 0.115933, acc 0.984375
2020-02-09T01:27:01.674575: step 1290, loss 0.164933, acc 0.953125
2020-02-09T01:27:01.802753: step 1291, loss 0.387844, acc 0.84375
2020-02-09T01:27:01.929823: step 1292, loss 0.232026, acc 0.90625
2020-02-09T01:27:02.055204: step 1293, loss 0.205712, acc 0.9375
2020-02-09T01:27:02.184617: step 1294, loss 0.172795, acc 0.921875
2020-02-09T01:27:02.340521: step 1295, loss 0.174493, acc 0.9375
2020-02-09T01:27:02.487742: step 1296, loss 0.157149, acc 0.953125
2020-02-09T01:27:02.606876: step 1297, loss 0.239109, acc 0.890625
2020-02-09T01:27:02.744026: step 1298, loss 0.212522, acc 0.90625
2020-02-09T01:27:02.867473: step 1299, loss 0.181153, acc 0.890625
2020-02-09T01:27:02.996868: step 1300, loss 0.265211, acc 0.84375

Evaluation:
2020-02-09T01:27:03.198035: step 1300, loss 0.594929, acc 0.736398

Saved model checkpoint to /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model-1300

2020-02-09T01:27:04.802165: step 1301, loss 0.210831, acc 0.875
2020-02-09T01:27:04.936416: step 1302, loss 0.348553, acc 0.828125
2020-02-09T01:27:05.057818: step 1303, loss 0.214472, acc 0.890625
2020-02-09T01:27:05.188687: step 1304, loss 0.198014, acc 0.921875
2020-02-09T01:27:05.311250: step 1305, loss 0.194791, acc 0.90625
2020-02-09T01:27:05.452121: step 1306, loss 0.277897, acc 0.859375
2020-02-09T01:27:05.590140: step 1307, loss 0.192181, acc 0.9375
2020-02-09T01:27:05.795076: step 1308, loss 0.222169, acc 0.9375
2020-02-09T01:27:05.951370: step 1309, loss 0.300322, acc 0.90625
2020-02-09T01:27:06.101259: step 1310, loss 0.275534, acc 0.890625
2020-02-09T01:27:06.232362: step 1311, loss 0.32569, acc 0.890625
2020-02-09T01:27:06.361503: step 1312, loss 0.216838, acc 0.921875
2020-02-09T01:27:06.488111: step 1313, loss 0.265452, acc 0.875
2020-02-09T01:27:06.631844: step 1314, loss 0.214957, acc 0.90625
2020-02-09T01:27:06.761008: step 1315, loss 0.208234, acc 0.890625
2020-02-09T01:27:06.891767: step 1316, loss 0.282901, acc 0.9375
2020-02-09T01:27:07.038688: step 1317, loss 0.304592, acc 0.859375
2020-02-09T01:27:07.184766: step 1318, loss 0.211181, acc 0.875
2020-02-09T01:27:07.321087: step 1319, loss 0.291107, acc 0.84375
2020-02-09T01:27:07.449075: step 1320, loss 0.204292, acc 0.890625
2020-02-09T01:27:07.575600: step 1321, loss 0.193881, acc 0.921875
2020-02-09T01:27:07.701652: step 1322, loss 0.183646, acc 0.9375
2020-02-09T01:27:07.830707: step 1323, loss 0.205026, acc 0.90625
2020-02-09T01:27:07.998661: step 1324, loss 0.402153, acc 0.828125
2020-02-09T01:27:08.130786: step 1325, loss 0.226804, acc 0.90625
2020-02-09T01:27:08.259096: step 1326, loss 0.238359, acc 0.921875
2020-02-09T01:27:08.406612: step 1327, loss 0.229456, acc 0.890625
2020-02-09T01:27:08.550029: step 1328, loss 0.175136, acc 0.9375
2020-02-09T01:27:08.675935: step 1329, loss 0.187909, acc 0.921875
2020-02-09T01:27:08.804635: step 1330, loss 0.123113, acc 0.96875
2020-02-09T01:27:08.969924: step 1331, loss 0.222734, acc 0.875
2020-02-09T01:27:09.125402: step 1332, loss 0.216124, acc 0.921875
2020-02-09T01:27:09.280811: step 1333, loss 0.141453, acc 0.9375
2020-02-09T01:27:09.420105: step 1334, loss 0.207217, acc 0.921875
2020-02-09T01:27:09.543897: step 1335, loss 0.228646, acc 0.90625
2020-02-09T01:27:09.669818: step 1336, loss 0.244486, acc 0.890625
2020-02-09T01:27:09.796594: step 1337, loss 0.197431, acc 0.90625
2020-02-09T01:27:09.936548: step 1338, loss 0.25387, acc 0.859375
2020-02-09T01:27:10.072825: step 1339, loss 0.335484, acc 0.90625
2020-02-09T01:27:10.205629: step 1340, loss 0.231061, acc 0.90625
2020-02-09T01:27:10.339041: step 1341, loss 0.301793, acc 0.921875
2020-02-09T01:27:10.465169: step 1342, loss 0.244342, acc 0.90625
2020-02-09T01:27:10.588298: step 1343, loss 0.311534, acc 0.828125
2020-02-09T01:27:10.718011: step 1344, loss 0.16626, acc 0.890625
2020-02-09T01:27:10.851641: step 1345, loss 0.372813, acc 0.859375
2020-02-09T01:27:11.000421: step 1346, loss 0.157563, acc 0.9375
2020-02-09T01:27:11.141225: step 1347, loss 0.272445, acc 0.84375
2020-02-09T01:27:11.272317: step 1348, loss 0.192382, acc 0.921875
2020-02-09T01:27:11.392701: step 1349, loss 0.159345, acc 0.953125
2020-02-09T01:27:11.512636: step 1350, loss 0.177699, acc 0.9
2020-02-09T01:27:11.635470: step 1351, loss 0.261458, acc 0.90625
2020-02-09T01:27:11.787937: step 1352, loss 0.244865, acc 0.875
2020-02-09T01:27:11.941175: step 1353, loss 0.154455, acc 0.96875
2020-02-09T01:27:12.089148: step 1354, loss 0.111002, acc 0.96875
2020-02-09T01:27:12.224352: step 1355, loss 0.239532, acc 0.921875
2020-02-09T01:27:12.382731: step 1356, loss 0.186812, acc 0.9375
2020-02-09T01:27:12.538222: step 1357, loss 0.144155, acc 0.921875
2020-02-09T01:27:12.693515: step 1358, loss 0.265815, acc 0.90625
2020-02-09T01:27:12.856221: step 1359, loss 0.156796, acc 0.953125
2020-02-09T01:27:13.011907: step 1360, loss 0.2114, acc 0.890625
2020-02-09T01:27:13.168804: step 1361, loss 0.211312, acc 0.859375
2020-02-09T01:27:13.404747: step 1362, loss 0.251094, acc 0.890625
2020-02-09T01:27:13.623184: step 1363, loss 0.281061, acc 0.890625
2020-02-09T01:27:13.816615: step 1364, loss 0.195765, acc 0.890625
2020-02-09T01:27:13.983398: step 1365, loss 0.1999, acc 0.90625
2020-02-09T01:27:14.315276: step 1366, loss 0.294312, acc 0.8125
2020-02-09T01:27:14.445917: step 1367, loss 0.200758, acc 0.9375
2020-02-09T01:27:14.577351: step 1368, loss 0.222213, acc 0.90625
2020-02-09T01:27:14.699724: step 1369, loss 0.184442, acc 0.9375
2020-02-09T01:27:14.833674: step 1370, loss 0.19979, acc 0.953125
2020-02-09T01:27:14.963034: step 1371, loss 0.134362, acc 0.953125
2020-02-09T01:27:15.104244: step 1372, loss 0.190052, acc 0.921875
2020-02-09T01:27:15.233661: step 1373, loss 0.254624, acc 0.890625
2020-02-09T01:27:15.375281: step 1374, loss 0.177166, acc 0.90625
2020-02-09T01:27:15.507717: step 1375, loss 0.162381, acc 0.9375
2020-02-09T01:27:15.628973: step 1376, loss 0.360204, acc 0.828125
2020-02-09T01:27:15.757200: step 1377, loss 0.229789, acc 0.921875
2020-02-09T01:27:15.892019: step 1378, loss 0.131661, acc 0.96875
2020-02-09T01:27:16.027205: step 1379, loss 0.134132, acc 0.96875
2020-02-09T01:27:16.154173: step 1380, loss 0.104331, acc 0.953125
2020-02-09T01:27:16.280486: step 1381, loss 0.239839, acc 0.90625
2020-02-09T01:27:16.417648: step 1382, loss 0.272362, acc 0.875
2020-02-09T01:27:16.543179: step 1383, loss 0.171455, acc 0.9375
2020-02-09T01:27:16.676892: step 1384, loss 0.279651, acc 0.875
2020-02-09T01:27:16.830069: step 1385, loss 0.183097, acc 0.9375
2020-02-09T01:27:16.970380: step 1386, loss 0.196671, acc 0.890625
2020-02-09T01:27:17.103992: step 1387, loss 0.145149, acc 0.921875
2020-02-09T01:27:17.245986: step 1388, loss 0.162201, acc 0.953125
2020-02-09T01:27:17.377673: step 1389, loss 0.112972, acc 0.984375
2020-02-09T01:27:17.511638: step 1390, loss 0.205702, acc 0.9375
2020-02-09T01:27:17.629689: step 1391, loss 0.26674, acc 0.875
2020-02-09T01:27:17.753143: step 1392, loss 0.0807473, acc 1
2020-02-09T01:27:17.879811: step 1393, loss 0.141577, acc 0.96875
2020-02-09T01:27:18.004583: step 1394, loss 0.124755, acc 0.9375
2020-02-09T01:27:18.128523: step 1395, loss 0.155584, acc 0.96875
2020-02-09T01:27:18.258585: step 1396, loss 0.308077, acc 0.890625
2020-02-09T01:27:18.386085: step 1397, loss 0.131961, acc 0.953125
2020-02-09T01:27:18.513267: step 1398, loss 0.0946229, acc 0.96875
2020-02-09T01:27:18.644593: step 1399, loss 0.204926, acc 0.90625
2020-02-09T01:27:18.793667: step 1400, loss 0.164461, acc 0.921875

Evaluation:
2020-02-09T01:27:18.990446: step 1400, loss 0.63308, acc 0.723265

Traceback (most recent call last):
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
    return fn(*args)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    target_list, run_metadata)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints; No such file or directory
	 [[{{node save/SaveV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 1176, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints; No such file or directory
	 [[node save/SaveV2 (defined at /Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save/SaveV2':
  File "train.py", line 196, in <module>
    tf.app.run()
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "train.py", line 193, in main
    train(x_train, y_train, vocab_processor, x_dev, y_dev)
  File "train.py", line 134, in train
    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 505, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 206, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 122, in save_op
    tensors)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1946, in save_v2
    name=name)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 196, in <module>
    tf.app.run()
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "train.py", line 193, in main
    train(x_train, y_train, vocab_processor, x_dev, y_dev)
  File "train.py", line 188, in train
    path = saver.save(sess, checkpoint_prefix, global_step=current_step)
  File "/Volumes/Transcend/opt/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 1193, in save
    raise exc
ValueError: Parent directory of /Volumes/Transcend/Mutation/CNN-TF/runs/1581182568/checkpoints/model doesn't exist, can't save.
